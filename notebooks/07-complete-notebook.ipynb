{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Importing the Dataset"
      ],
      "metadata": {
        "id": "OFOOky8kQWmB"
      },
      "id": "OFOOky8kQWmB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3ycIUwGPR55"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os, glob, pandas as pd"
      ],
      "id": "k3ycIUwGPR55"
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"andrewmvd/heart-failure-clinical-data\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "eBZ-hjglPR56"
      },
      "execution_count": null,
      "outputs": [],
      "id": "eBZ-hjglPR56"
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(os.path.join(path, \"heart_failure_clinical_records_dataset.csv\"))\n",
        "df.head()"
      ],
      "metadata": {
        "id": "kVwvzlrDPR57"
      },
      "execution_count": null,
      "outputs": [],
      "id": "kVwvzlrDPR57"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking for any Null Values (Data Cleaning)"
      ],
      "metadata": {
        "id": "6J-HSHNuPR57"
      },
      "id": "6J-HSHNuPR57"
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "sZwpRgN3PR57"
      },
      "execution_count": null,
      "outputs": [],
      "id": "sZwpRgN3PR57"
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "Ee2LQR4uPR57"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Ee2LQR4uPR57"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking for Outliers (Data Cleaning)"
      ],
      "metadata": {
        "id": "C86MaCY4PR57"
      },
      "id": "C86MaCY4PR57"
    },
    {
      "cell_type": "code",
      "source": [
        "non_binary_columns = ['creatinine_phosphokinase', 'ejection_fraction', 'platelets', 'serum_creatinine', 'serum_sodium']\n",
        "quartile_1 = df[non_binary_columns].quantile(0.25)\n",
        "quartile_3 = df[non_binary_columns].quantile(0.75)\n",
        "iqr = quartile_3 - quartile_1\n",
        "lower_bound = quartile_1 - 1.5 * iqr\n",
        "upper_bound = quartile_3 + 1.5 * iqr\n",
        "\n",
        "outliers = (df[non_binary_columns] < lower_bound) | (df[non_binary_columns] > upper_bound)\n",
        "outliers_count = outliers.sum()\n",
        "\n",
        "print(\"Number of outliers in each column:\")\n",
        "print(outliers_count)\n"
      ],
      "metadata": {
        "id": "udrWgs0jPR57"
      },
      "execution_count": null,
      "outputs": [],
      "id": "udrWgs0jPR57"
    },
    {
      "cell_type": "code",
      "source": [
        "# helper function for evaluation\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
        "\n",
        "def evaluate_model(y_true, y_pred, y_proba=None, model_name=\"Model\"):\n",
        "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
        "    print(\"Precision:\", precision_score(y_true, y_pred))\n",
        "    print(\"Recall:\", recall_score(y_true, y_pred))\n",
        "    print(\"F1 Score:\", f1_score(y_true, y_pred))\n",
        "\n",
        "    if y_proba is not None:\n",
        "        print(\"ROC-AUC:\", roc_auc_score(y_true, y_proba))\n",
        "\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(confusion_matrix(y_true, y_pred))"
      ],
      "metadata": {
        "id": "5fVzGylGQVOh"
      },
      "id": "5fVzGylGQVOh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "9de904cb",
      "metadata": {
        "id": "9de904cb"
      },
      "source": [
        "# EDA Notebook 01\n",
        "We will be using this notebook for preliminary EDA to understand the data better before we start fitting models and analyzing results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45ec03e4",
      "metadata": {
        "id": "45ec03e4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set the style of the plots\n",
        "sns.set_style('darkgrid')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f5e1c01",
      "metadata": {
        "id": "8f5e1c01"
      },
      "outputs": [],
      "source": [
        "print(df.head(1))\n",
        "print('='*100)\n",
        "print(df.info())\n",
        "print('='*100)\n",
        "print(df.describe())\n",
        "print('='*100)\n",
        "# Check for missing values\n",
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff7c4ab6",
      "metadata": {
        "id": "ff7c4ab6"
      },
      "source": [
        "### Clean Data (some Preprocessing)\n",
        "Note: we should still use non scaled (raw) data for initial EDA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6291bf48",
      "metadata": {
        "id": "6291bf48"
      },
      "outputs": [],
      "source": [
        "continuous_features = ['age', 'creatinine_phosphokinase', 'ejection_fraction', 'platelets', 'serum_creatinine', 'serum_sodium', 'time']\n",
        "categorical_features = ['anaemia', 'diabetes', 'high_blood_pressure', 'sex', 'smoking']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a406e058",
      "metadata": {
        "id": "a406e058"
      },
      "outputs": [],
      "source": [
        "# Remove Outliers\n",
        "quantile_1 = df[continuous_features].quantile(0.25)\n",
        "quantile_3 = df[continuous_features].quantile(0.75)\n",
        "iqr = quantile_3 - quantile_1\n",
        "lower_bound = quantile_1 - 1.5 * iqr\n",
        "upper_bound = quantile_3 + 1.5 * iqr\n",
        "\n",
        "outliers = (df[continuous_features] < lower_bound) | (df[continuous_features] > upper_bound)\n",
        "\n",
        "print(\"Outliers: \")\n",
        "outliers.sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdfa9376",
      "metadata": {
        "id": "fdfa9376"
      },
      "outputs": [],
      "source": [
        "rows_w_outliers = outliers.any(axis=1)\n",
        "df_clean = df[~rows_w_outliers] # return dataframe without outliers\n",
        "\n",
        "df_clean.shape\n",
        "df = df_clean.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce89be38",
      "metadata": {
        "id": "ce89be38"
      },
      "outputs": [],
      "source": [
        "# save cleaned data to csv\n",
        "# df.to_csv('../data/heart_failure_clinical_records_dataset_cleaned.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46125c59",
      "metadata": {
        "id": "46125c59"
      },
      "source": [
        "There are not any missing values, we should be okay to proceed to EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "652596fc",
      "metadata": {
        "id": "652596fc"
      },
      "source": [
        "### Explore DEATH_EVENT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6885a102",
      "metadata": {
        "id": "6885a102"
      },
      "outputs": [],
      "source": [
        "# Explore death event distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(x='DEATH_EVENT', data=df)\n",
        "plt.title('Distribution of Death Events')\n",
        "plt.xlabel('Death Event')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks([0, 1], ['Survived', 'Died'])\n",
        "plt.show()\n",
        "print(\"Percentage of DEATH_EVENT:\")\n",
        "print(df['DEATH_EVENT'].value_counts(normalize=True)*100)\n",
        "imbalance_ratio = df['DEATH_EVENT'].value_counts(normalize=True)[0] / df['DEATH_EVENT'].value_counts(normalize=True)[1]\n",
        "print(f\"\\nImbalance ratio: {imbalance_ratio:.2f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b24977b9",
      "metadata": {
        "id": "b24977b9"
      },
      "source": [
        "The dataset shows moderate class imbalance, with approximately 68% survival rate (203 patients) and 32% mortality rate (96 patients). The imbalance ratio is about 2:1, which is moderate - not severe enough to require aggressive resampling, but we should consider using stratified cross-validation and monitoring precision/recall metrics in addition to accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fcf8b55",
      "metadata": {
        "id": "7fcf8b55"
      },
      "outputs": [],
      "source": [
        "# Death Rate between Male and Female:\n",
        "\n",
        "male = df.loc[df['sex'] == 1, 'DEATH_EVENT']\n",
        "female = df.loc[df['sex'] == 0, 'DEATH_EVENT']\n",
        "\n",
        "print(\"Male Death Rate\", male.mean())\n",
        "print(\"Female Death Rate\", female.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10d48381",
      "metadata": {
        "id": "10d48381"
      },
      "source": [
        "### Univariate EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68e0a8be",
      "metadata": {
        "id": "68e0a8be"
      },
      "outputs": [],
      "source": [
        "# Create histograms and KDE for continuous features\n",
        "fix, axis = plt.subplots(3,3, figsize=(15,12))\n",
        "axis = axis.flatten()\n",
        "\n",
        "for i, feature in enumerate(continuous_features):\n",
        "    sns.histplot(\n",
        "        df[feature],\n",
        "        ax=axis[i],\n",
        "        kde=True,\n",
        "        bins=30)\n",
        "    axis[i].set_title(f'Histogram and KDE of {feature}')\n",
        "    axis[i].set_xlabel(feature)\n",
        "    axis[i].set_ylabel('Frequency')\n",
        "\n",
        "    skew = df[feature].skew()\n",
        "    axis[i].text(0.7, 0.9, f'Skewness: {skew:.2f}',\n",
        "        transform=axis[i].transAxes,\n",
        "        bbox=dict(boxstyle='round',facecolor='wheat',alpha=0.5))\n",
        "\n",
        "axis[-2].remove()\n",
        "axis[-1].remove()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "208a7e19",
      "metadata": {
        "id": "208a7e19"
      },
      "source": [
        "### Analysis of Feature Distributions (After Outlier Removal)\n",
        "\n",
        "### Skewness Interpretation\n",
        "\n",
        "**Highly Symmetric (|skew| < 0.5):**\n",
        "- **time** (skew = 0.06): Nearly perfectly symmetric distribution\n",
        "- **serum_sodium** (skew = -0.11): Almost perfectly symmetric, well-centered distribution\n",
        "- **platelets** (skew = 0.25): Very symmetric, minimal skew\n",
        "- **age** (skew = 0.37): Slight right skew, relatively normal distribution centered around 60 years\n",
        "- **ejection_fraction** (skew = 0.38): Slight right skew, appears somewhat uniform with potential subgroups (normal vs reduced ejection fraction)\n",
        "\n",
        "**Moderately Skewed (0.5 < |skew| < 2):**\n",
        "- **serum_creatinine** (skew = 0.94): Moderate right skew, most patients have normal kidney function with some elevated values\n",
        "- **creatinine_phosphokinase** (skew = 0.97): Moderate right skew, previously highly skewed but much improved after outlier removal\n",
        "\n",
        "### Key Observations\n",
        "\n",
        "**Impact of Outlier Removal:**\n",
        "1. **Dramatic improvement in skewness**: CPK and serum_creatinine went from extremely skewed (4.46) to moderately skewed (~0.95), making them much more suitable for parametric models\n",
        "2. **Better normality**: Most features now fall within the fairly symmetric range (|skew| < 0.5), which is ideal for many machine learning algorithms\n",
        "3. **Preserved clinical validity**: Distributions still reflect real medical patterns without extreme outliers\n",
        "\n",
        "**Remaining Characteristics:**\n",
        "- **Scale differences**: Features still span different ranges (will require normalization/standardization for modeling)\n",
        "- **Potential subgroups**: Ejection fraction still shows evidence of two patient populations (normal vs reduced EF)\n",
        "- **Data quality**: After outlier removal, the dataset is cleaner and more suitable for modeling while maintaining clinical relevance\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dd323ff",
      "metadata": {
        "id": "1dd323ff"
      },
      "source": [
        "### Bivariate EDA: Features vs Target (DEATH_EVENT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09bce73b",
      "metadata": {
        "id": "09bce73b"
      },
      "outputs": [],
      "source": [
        "# Violin plots for key continuous features (showing distribution shape)\n",
        "key_features = ['ejection_fraction', 'serum_creatinine', 'time', 'age']\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, feature in enumerate(key_features):\n",
        "    sns.violinplot(data=df, x='DEATH_EVENT', y=feature, ax=axes[i], palette='muted', inner='quartile')\n",
        "    axes[i].set_title(f'Violin Plot: {feature} vs DEATH_EVENT', fontsize=12, fontweight='bold')\n",
        "    axes[i].set_xlabel('Death Event (0=Survived, 1=Died)')\n",
        "    axes[i].set_ylabel(feature)\n",
        "    axes[i].set_xticklabels(['Survived', 'Died'])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f96fc7f",
      "metadata": {
        "id": "6f96fc7f"
      },
      "source": [
        "### Analysis\n",
        "#### Ejection Fraction:\n",
        "Survived group:  \n",
        "\n",
        "- Centered around 40–50%, with many patients between 35–60%.  \n",
        "- Distribution is wider (more patients with moderately healthy function).  \n",
        "\n",
        "Dead group:  \n",
        "\n",
        "- Strongly centered near 25–30%.  \n",
        "- Very few patients with healthy ejection fractions (>40%).  \n",
        "- Shape is narrow and skewed toward low EF.  \n",
        "\n",
        "*Conclusion:*  \n",
        "Lower ejection fraction is associated with death.  \n",
        "This feature will likely be one of the top predictors.  \n",
        "\n",
        "#### Serum Creatinine\n",
        "\n",
        "Survived:\n",
        "\n",
        "- Most patients have creatinine around 0.8–1.2 mg/dL (normal range).\n",
        "- Very tight distribution.\n",
        "\n",
        "Died:\n",
        "\n",
        "- Distribution shifts upward: many near 1.5–2.0 mg/dL.\n",
        "- Much wider spread; more high-creatinine outliers.\n",
        "\n",
        "*Conclusion:*  \n",
        "Higher serum creatinine (worse kidney function) is strongly associated with death.\n",
        "Expecting this feature to rank highly in model importance.\n",
        "\n",
        "#### Age\n",
        "- Patients who died tend to be older, which implies that age is a meaninful risk factor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18d2e798",
      "metadata": {
        "id": "18d2e798"
      },
      "outputs": [],
      "source": [
        "# Grouped bar charts: Death rate by categorical features\n",
        "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, feature in enumerate(categorical_features):\n",
        "    death_rate = df.groupby(feature)['DEATH_EVENT'].agg(['sum', 'count', 'mean'])\n",
        "    death_rate['survival_rate'] = 1 - death_rate['mean']\n",
        "\n",
        "    x = death_rate.index.astype(str)\n",
        "    width = 0.35\n",
        "    x_pos = np.arange(len(x))\n",
        "\n",
        "    axes[i].bar(x_pos - width/2, death_rate['survival_rate'] * 100, width,\n",
        "                label='Survived %', color='skyblue', edgecolor='black')\n",
        "    axes[i].bar(x_pos + width/2, death_rate['mean'] * 100, width,\n",
        "                label='Died %', color='salmon', edgecolor='black')\n",
        "\n",
        "    axes[i].set_xlabel(feature)\n",
        "    axes[i].set_ylabel('Percentage')\n",
        "    axes[i].set_title(f'Death Rate by {feature}')\n",
        "    axes[i].set_xticks(x_pos)\n",
        "    axes[i].set_xticklabels([f'{feature}={val}' for val in x])\n",
        "    axes[i].legend()\n",
        "    axes[i].set_ylim(0, 100)\n",
        "\n",
        "    for j, (surv, died) in enumerate(zip(death_rate['survival_rate'] * 100, death_rate['mean'] * 100)):\n",
        "        axes[i].text(j - width/2, surv + 2, f'{surv:.1f}%', ha='center', fontsize=9)\n",
        "        axes[i].text(j + width/2, died + 2, f'{died:.1f}%', ha='center', fontsize=9)\n",
        "\n",
        "axes[-1].remove()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0bcbdc4",
      "metadata": {
        "id": "a0bcbdc4"
      },
      "outputs": [],
      "source": [
        "# Statistical summary: Death rates by categorical features\n",
        "print(\"Death Rates by Categorical Features:\\n\" + \"=\"*60)\n",
        "for feature in categorical_features:\n",
        "    print(f\"\\n{feature}:\")\n",
        "    cross_tab = pd.crosstab(df[feature], df['DEATH_EVENT'], normalize='index') * 100\n",
        "    cross_tab.columns = ['Survived %', 'Died %']\n",
        "    print(cross_tab.round(2))\n",
        "\n",
        "    # Chi-square test for independence\n",
        "    from scipy.stats import chi2_contingency\n",
        "    contingency_table = pd.crosstab(df[feature], df['DEATH_EVENT'])\n",
        "    chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
        "    print(f\"  Chi-square test: χ² = {chi2:.2f}, p-value = {p_value:.4f} {'***' if p_value < 0.001 else '**' if p_value < 0.01 else '*' if p_value < 0.05 else 'ns'}\")\n",
        "    print(\"-\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0093b87b",
      "metadata": {
        "id": "0093b87b"
      },
      "source": [
        "### Correlation Analysis & Feature Interactions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55a0089d",
      "metadata": {
        "id": "55a0089d"
      },
      "outputs": [],
      "source": [
        "# Compute Pearson correlation matrix for all numeric features\n",
        "correlation_matrix = df.corr(method='pearson')\n",
        "\n",
        "# Create heatmap\n",
        "plt.figure(figsize=(14, 12))\n",
        "sns.heatmap(correlation_matrix,\n",
        "            annot=True,\n",
        "            fmt='.2f',\n",
        "            cmap='coolwarm',\n",
        "            center=0,\n",
        "            square=True,\n",
        "            linewidths=1,\n",
        "            cbar_kws={\"shrink\": 0.8})\n",
        "plt.title('Correlation Matrix (Pearson) - All Features', fontsize=16, fontweight='bold', pad=20)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e118149f",
      "metadata": {
        "id": "e118149f"
      },
      "outputs": [],
      "source": [
        "# Correlation with target variable (DEATH_EVENT)\n",
        "target_corr = correlation_matrix['DEATH_EVENT'].drop('DEATH_EVENT').sort_values(ascending=False)\n",
        "\n",
        "print(\"Correlations with DEATH_EVENT (sorted by absolute value):\\n\")\n",
        "print(\"=\"*60)\n",
        "for feature, corr_value in target_corr.items():\n",
        "    strength = 'Strong' if abs(corr_value) > 0.5 else 'Moderate' if abs(corr_value) > 0.3 else 'Weak'\n",
        "    direction = 'positive' if corr_value > 0 else 'negative'\n",
        "    print(f\"{feature:30s}: {corr_value:6.3f} ({strength} {direction})\")\n",
        "\n",
        "# Visualize correlations with DEATH_EVENT\n",
        "plt.figure(figsize=(10, 8))\n",
        "colors = ['red' if x < 0 else 'green' for x in target_corr.values]\n",
        "target_corr.plot(kind='barh', color=colors, edgecolor='black')\n",
        "plt.xlabel('Correlation Coefficient', fontsize=12)\n",
        "plt.ylabel('Features', fontsize=12)\n",
        "plt.title('Feature Correlations with DEATH_EVENT', fontsize=14, fontweight='bold')\n",
        "plt.axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
        "plt.axvline(x=0.3, color='gray', linestyle='--', linewidth=0.5, alpha=0.7, label='Moderate threshold (±0.3)')\n",
        "plt.axvline(x=-0.3, color='gray', linestyle='--', linewidth=0.5, alpha=0.7)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dca3478c",
      "metadata": {
        "id": "dca3478c"
      },
      "outputs": [],
      "source": [
        "# Check for multicollinearity among features (excluding target)\n",
        "feature_corr = correlation_matrix.drop('DEATH_EVENT', axis=0).drop('DEATH_EVENT', axis=1)\n",
        "\n",
        "# Find pairs with high correlation (potential multicollinearity)\n",
        "print(\"\\nHigh Feature-to-Feature Correlations (|r| > 0.5):\")\n",
        "print(\"=\"*60)\n",
        "high_corr_pairs = []\n",
        "for i in range(len(feature_corr.columns)):\n",
        "    for j in range(i+1, len(feature_corr.columns)):\n",
        "        if abs(feature_corr.iloc[i, j]) > 0.5:\n",
        "            high_corr_pairs.append((feature_corr.columns[i],\n",
        "                                   feature_corr.columns[j],\n",
        "                                   feature_corr.iloc[i, j]))\n",
        "            print(f\"{feature_corr.columns[i]:30s} <-> {feature_corr.columns[j]:30s}: {feature_corr.iloc[i, j]:6.3f}\")\n",
        "\n",
        "if not high_corr_pairs:\n",
        "    print(\"No strong feature-to-feature correlations detected (good - low multicollinearity)\")\n",
        "else:\n",
        "    print(f\"\\nFound {len(high_corr_pairs)} pairs with potential multicollinearity\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a3132dd",
      "metadata": {
        "id": "3a3132dd"
      },
      "source": [
        "### Note on PCA:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3df2b07b",
      "metadata": {
        "id": "3df2b07b"
      },
      "source": [
        "We did not apply PCA because the dataset contains only twelve features, all of which are clinically meaningful and interpretable. PCA is most useful for high-dimensional datasets or when features are highly correlated, neither of which applies here. Additionally, PCA removes interpretability by transforming predictors into abstract components, and several of our models (such as logistic regression with regularization and tree-based methods) already handle correlation and noise effectively without dimensionality reduction.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ohLUFT79Odpp"
      },
      "id": "ohLUFT79Odpp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89e4ab7a"
      },
      "source": [
        "# Data Cleaning & Preprocessing Notebook 02\n",
        "This is where we will do any cleaning and standardizations to prepare for using ML models to predict DEATH_EVENT.  \n",
        "\n",
        "*Note:* Some of these steps are redundant/unnecessary but are show for 'full lifecycle coverage'. Sometimes these two note books (ie. 01,02) are combined into 1 for simplicity.  "
      ],
      "id": "89e4ab7a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44ad8605"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Set the style of the plots\n",
        "sns.set_style('darkgrid')"
      ],
      "id": "44ad8605"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4d31f9a9"
      },
      "outputs": [],
      "source": [
        "continuous_features = ['age', 'creatinine_phosphokinase', 'ejection_fraction', 'platelets', 'serum_creatinine', 'serum_sodium', 'time']\n",
        "categorical_features = ['anaemia', 'diabetes', 'high_blood_pressure', 'sex', 'smoking']"
      ],
      "id": "4d31f9a9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "406c3385"
      },
      "outputs": [],
      "source": [
        "# Detect and handle missing values (already checked)\n",
        "missing_values = df.isnull().sum()\n",
        "print(\"Missing values:\")\n",
        "print(missing_values)\n",
        "\n",
        "# Handle missing values - None\n"
      ],
      "id": "406c3385"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1454f986"
      },
      "outputs": [],
      "source": [
        "# Remove Outliers (already checked)\n",
        "quantile_1 = df[continuous_features].quantile(0.25)\n",
        "quantile_3 = df[continuous_features].quantile(0.75)\n",
        "iqr = quantile_3 - quantile_1\n",
        "lower_bound = quantile_1 - 1.5 * iqr\n",
        "upper_bound = quantile_3 + 1.5 * iqr\n",
        "\n",
        "outliers = (df[continuous_features] < lower_bound) | (df[continuous_features] > upper_bound)\n",
        "\n",
        "print(\"Outliers: \")\n",
        "outliers.sum()\n",
        "# Handle Outliers\n",
        "# Will skip this step as we did it in 01 EDA notebook\n",
        "# (ignoring the 3 extra outliers since they are being calculate on the cleaned dataset)"
      ],
      "id": "1454f986"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08ea8508"
      },
      "source": [
        "### Standardization and Train Split\n",
        "Some of our ML models that we will be using require standardization of features. So we will first split our data, then standardize it.  \n",
        "\n",
        "Process:  \n",
        "- We will standardize only the continuous numerical features (age, creatinine_phosphokinase, ejection_fraction, platelets, serum_creatinine, serum_sodium, time) using StandardScaler.  \n",
        "\n",
        "- Binary 0/1 variables were left unchanged.\n",
        "\n",
        "- Standardization was applied in a scikit-learn Pipeline, fit only on the training data, and then applied to the test set.   \n",
        "\n",
        "- This was necessary for scale-sensitive models such as logistic regression, SVM, k-NN, and neural networks.\n",
        "\n",
        "- Tree-based models do not require scaling, but using a consistent preprocessing pipeline prevents data leakage and maintains comparability across models.\n",
        "\n"
      ],
      "id": "08ea8508"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ba55b07"
      },
      "source": [
        "### Models That Require Feature Scaling\n",
        "\n",
        "| **Model** | **Why It Needs Scaling** |\n",
        "|----------|---------------------------|\n",
        "| **Logistic Regression** | L2 regularization assumes all features are on the same scale; otherwise coefficients are penalized inconsistently. |\n",
        "| **SVM (Linear, RBF kernels)** | Uses distance and dot products; features with larger magnitude dominate the decision boundary. |\n",
        "| **k-NN** | Distance-based algorithm—unscaled features distort nearest-neighbor calculations. |\n",
        "| **Neural Networks (PyTorch)** | Gradient-based optimization converges faster when inputs have similar scale; prevents exploding/vanishing gradients. |\n",
        "| **PCA** | Computes variance along components; features with larger variance dominate unless scaled. |\n"
      ],
      "id": "1ba55b07"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bff77f4"
      },
      "outputs": [],
      "source": [
        "# Train, Validation, Test Split\n",
        "X = df.drop('DEATH_EVENT', axis=1)\n",
        "y = df['DEATH_EVENT']\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
      ],
      "id": "8bff77f4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "da4f4c5f"
      },
      "outputs": [],
      "source": [
        "# Column Transformer + Pipeline\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), continuous_features),\n",
        "        ('cat', 'passthrough', categorical_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Example pipeline with logistic regression, will redo this part in the ML notebook\n",
        "lr_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(penalty='l2', C=0.1, solver='liblinear',class_weight='balanced'))]\n",
        ")"
      ],
      "id": "da4f4c5f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e054cf2e"
      },
      "outputs": [],
      "source": [
        "# Verify Feature Scaling\n",
        "X_train_scaled = preprocessor.fit_transform(X_train)\n",
        "print(\"Mean: \\n\", X_train_scaled.mean(axis=0))\n",
        "print(\"Std: \\n\", X_train_scaled.std(axis=0))\n"
      ],
      "id": "e054cf2e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8361c159"
      },
      "source": [
        "Since for continous features the mean ~ 0 and std ~ 1, scaling is working as intended!"
      ],
      "id": "8361c159"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "122f7708"
      },
      "source": [
        "### Note:\n",
        "The imbalance in the dataset in DEATH_EVENT is about 2.67:1 to died:survived. The imbalance is mild and using a process such as SMOTE may add unneccessary noise and can hurt generalization."
      ],
      "id": "122f7708"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "671c1c56"
      },
      "outputs": [],
      "source": [
        "# Add data to CSVs for access in next notebook\n",
        "X_train.to_csv(\"X_train.csv\", index=False)\n",
        "X_val.to_csv(\"X_val.csv\", index=False)\n",
        "y_train.to_csv(\"y_train.csv\", index=False)\n",
        "y_val.to_csv(\"y_val.csv\", index=False)"
      ],
      "id": "671c1c56"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "638f3bca"
      },
      "source": [
        "# ML & Generating Predictions Notebook 03\n",
        "This notebook is where we will evaluate baseline and candidate models to see which model we should use as our primary.  \n",
        "\n",
        "*Note: We did not perform feature selection because the dataset contains only twelve clinically meaningful features, and removing any of them could discard useful non-linear or interaction effects. Simple correlation is not a reliable indicator of predictive value, especially in medical data. Several models we use, including Random Forests, Gradient Boosting, and regularized Logistic Regression, already perform built-in feature selection or regularization by down-weighting weaker predictors. Because these models naturally handle feature relevance, explicit feature elimination is unnecessary for this project. We will analyze feature importance in a future step*"
      ],
      "id": "638f3bca"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41a08353"
      },
      "outputs": [],
      "source": [
        "# Standard libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Preprocessing\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# ML Models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "\n",
        "# XGBoost\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Evaluation metrics\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    roc_auc_score,\n",
        "    roc_curve,\n",
        "    confusion_matrix,\n",
        "    ConfusionMatrixDisplay\n",
        ")\n",
        "\n",
        "# Set plot style\n",
        "sns.set_style('darkgrid')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n"
      ],
      "id": "41a08353"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a37db2d1"
      },
      "outputs": [],
      "source": [
        "# Results Dict\n",
        "all_results = {}"
      ],
      "id": "a37db2d1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19402df1"
      },
      "source": [
        "### We will create a baseline logistic regression for our actual models to outperform"
      ],
      "id": "19402df1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fc468c4"
      },
      "outputs": [],
      "source": [
        "continuous_features = ['age', 'creatinine_phosphokinase', 'ejection_fraction', 'platelets', 'serum_creatinine', 'serum_sodium', 'time']\n",
        "categorical_features = ['anaemia', 'diabetes', 'high_blood_pressure', 'sex', 'smoking']"
      ],
      "id": "8fc468c4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecc73bee"
      },
      "outputs": [],
      "source": [
        "# Load data - splits were created in previous notebook\n",
        "X_train = pd.read_csv(\"X_train.csv\")\n",
        "y_train = pd.read_csv(\"y_train.csv\").values.ravel()  # Convert to 1D array\n",
        "\n",
        "X_val = pd.read_csv(\"X_val.csv\")\n",
        "y_val = pd.read_csv(\"y_val.csv\").values.ravel()  # Convert to 1D array\n",
        "\n",
        "print(f\"Training set: {X_train.shape[0]} samples\")\n",
        "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
        "print(f\"Class distribution in training: {np.bincount(y_train)}\")\n",
        "print(f\"Class distribution in validation: {np.bincount(y_val)}\")"
      ],
      "id": "ecc73bee"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fec9530"
      },
      "outputs": [],
      "source": [
        "# create pipeline and run logistic regression\n",
        "\n",
        "# Column Transformer + Pipeline\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), continuous_features),\n",
        "        ('cat', 'passthrough', categorical_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Example pipeline with logistic regression, will redo this part in the ML notebook\n",
        "baseline_lr_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(solver='liblinear',class_weight='balanced'))]\n",
        ")"
      ],
      "id": "9fec9530"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2758d8dd"
      },
      "outputs": [],
      "source": [
        "# Train the baseline model\n",
        "baseline_lr_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on VALIDATION set\n",
        "y_val_pred = baseline_lr_pipeline.predict(X_val)\n",
        "y_val_pred_proba = baseline_lr_pipeline.predict_proba(X_val)[:, 1]  # Probability of death\n",
        "\n",
        "print(\"Baseline Logistic Regression trained successfully!\")\n"
      ],
      "id": "2758d8dd"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XZDHsHoyQLfo"
      },
      "id": "XZDHsHoyQLfo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6c03559c"
      },
      "outputs": [],
      "source": [
        "# Evaluate the Baseline Logistic Regression\n",
        "evaluate_model(y_val, y_val_pred, y_val_pred_proba, model_name=\"Baseline Logistic Regression\")\n"
      ],
      "id": "6c03559c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dd4c5684"
      },
      "outputs": [],
      "source": [
        "def plot_roc_curve(y_true, y_pred_proba, model_name=\"Model\"):\n",
        "    \"\"\"\n",
        "    Plot ROC curve for a single model.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    y_true : array-like\n",
        "        True labels\n",
        "    y_pred_proba : array-like\n",
        "        Predicted probabilities for positive class\n",
        "    model_name : str\n",
        "        Name of the model for display\n",
        "    \"\"\"\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba)\n",
        "    roc_auc = roc_auc_score(y_true, y_pred_proba)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'{model_name} (AUC = {roc_auc:.3f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier (AUC = 0.500)')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate', fontsize=12)\n",
        "    plt.ylabel('True Positive Rate (Recall)', fontsize=12)\n",
        "    plt.title(f'ROC Curve: {model_name}', fontsize=14, fontweight='bold')\n",
        "    plt.legend(loc=\"lower right\", fontsize=10)\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return roc_auc\n",
        "\n",
        "\n",
        "def plot_multiple_roc_curves(models_dict, y_true):\n",
        "    \"\"\"\n",
        "    Plot multiple ROC curves on the same plot for comparison.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    models_dict : dict\n",
        "        Dictionary with format: {'Model Name': y_pred_proba, ...}\n",
        "    y_true : array-like\n",
        "        True labels\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    colors = ['darkorange', 'green', 'red', 'purple', 'brown', 'pink', 'gray', 'olive', 'cyan']\n",
        "\n",
        "    for i, (model_name, y_pred_proba) in enumerate(models_dict.items()):\n",
        "        fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
        "        roc_auc = roc_auc_score(y_true, y_pred_proba)\n",
        "        color = colors[i % len(colors)]\n",
        "        plt.plot(fpr, tpr, lw=2, color=color, label=f'{model_name} (AUC = {roc_auc:.3f})')\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier (AUC = 0.500)')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate', fontsize=12)\n",
        "    plt.ylabel('True Positive Rate (Recall)', fontsize=12)\n",
        "    plt.title('ROC Curves: Model Comparison', fontsize=14, fontweight='bold')\n",
        "    plt.legend(loc=\"lower right\", fontsize=10)\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "id": "dd4c5684"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8e542f15"
      },
      "outputs": [],
      "source": [
        "# Plot ROC curve for baseline logistic regression\n",
        "plot_roc_curve(y_val, y_val_pred_proba, model_name=\"Baseline Logistic Regression\")\n"
      ],
      "id": "8e542f15"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aae705d7"
      },
      "source": [
        "### Now we will use more complex models"
      ],
      "id": "aae705d7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43aa9463"
      },
      "source": [
        "Notes:  \n",
        "- We used 5-fold stratified cross-validation for model selection because the dataset is relatively small (299 samples). Five folds provide a good balance between bias and variance, produce stable performance estimates, and avoid the computational cost of higher-fold CV while still giving more reliable results than a single train/validation split.\n",
        "- We will use GridSearchCV to return the best model for each and to tune hyperparameters"
      ],
      "id": "43aa9463"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aef7be28"
      },
      "source": [
        "### Logistic Regression\n",
        "We will begin with logistic regression. We will:\n",
        "- Tune the regularization strength of logistic regression using GridSearchCV over C ∈ {0.01, 0.1, 1, 10, 100} with 5-fold cross-validation and ROC–AUC as the scoring metric.\n",
        "- Selected the C_i that produced the highest mean cross-validated ROC-AUC on thr training set.\n",
        "- Refit the model with optimal C"
      ],
      "id": "aef7be28"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9e711786"
      },
      "outputs": [],
      "source": [
        "# Set up the pipeline\n",
        "lr_pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(solver='liblinear', class_weight='balanced', max_iter=1000, random_state=42,penalty='l2'))\n",
        "])\n",
        "\n",
        "# Define the hyperparameter grid - trying different regularization strengths\n",
        "c_vals = [i * 0.0001 for i in range(1, 15)]\n",
        "c_vals.append(0.01)\n",
        "c_vals.append(0.1)\n",
        "param_grid = {\n",
        "    'classifier__C': c_vals  # Lower C = stronger regularization\n",
        "}\n",
        "\n",
        "print(f\"Testing {len(c_vals)} different C values with 5-fold CV\")\n",
        "\n",
        "# Setup 5-fold cross validation (stratified to keep class balance in each fold)\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Create GridSearchCV - this will try all C values and pick the best one\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=lr_pipeline,\n",
        "    param_grid=param_grid,\n",
        "    cv=cv,\n",
        "    scoring='roc_auc',  # Most important metric for us\n",
        "    n_jobs=-1,  # Use all CPU cores\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "# Fit - this runs all the cross-validation\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nGrid search complete!\")\n"
      ],
      "id": "9e711786"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "469fa762"
      },
      "outputs": [],
      "source": [
        "print(\"Best parameters found:\")\n",
        "print(grid_search.best_params_)\n",
        "print(f\"\\nBest cross-validated ROC-AUC score: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Look at all the results to see how each C performed\n",
        "results_df = pd.DataFrame(grid_search.cv_results_)\n",
        "print(\"\\nAll cross-validation results:\")\n",
        "print(results_df[['param_classifier__C', 'mean_test_score', 'std_test_score']].sort_values('mean_test_score', ascending=False))\n"
      ],
      "id": "469fa762"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2667906"
      },
      "outputs": [],
      "source": [
        "# Visualize how performance changes with C\n",
        "plt.figure(figsize=(10, 6))\n",
        "mean_scores = results_df['mean_test_score'].values\n",
        "std_scores = results_df['std_test_score'].values\n",
        "\n",
        "plt.semilogx(c_vals, mean_scores, 'o-', linewidth=2, markersize=8, label='Mean ROC-AUC')\n",
        "plt.fill_between(c_vals, mean_scores - std_scores, mean_scores + std_scores, alpha=0.2, label='± 1 std dev')\n",
        "plt.xlabel('Regularization Parameter (C)', fontsize=12)\n",
        "plt.ylabel('Cross-Validated ROC-AUC', fontsize=12)\n",
        "plt.title('Logistic Regression: Hyperparameter Tuning Results', fontsize=14, fontweight='bold')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Interesting to see - lower C means more regularization (simpler model)\n",
        "# Higher C means less regularization (more complex model)\n"
      ],
      "id": "f2667906"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "891dde1d"
      },
      "outputs": [],
      "source": [
        "# The best model is already refitted on the full training set by GridSearchCV\n",
        "# We can access it directly\n",
        "tuned_lr_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on validation set\n",
        "y_val_pred_tuned = tuned_lr_model.predict(X_val)\n",
        "y_val_pred_proba_tuned = tuned_lr_model.predict_proba(X_val)[:, 1]\n",
        "\n",
        "print(\"Tuned Logistic Regression ready for evaluation!\")\n"
      ],
      "id": "891dde1d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cb56aa1f"
      },
      "outputs": [],
      "source": [
        "# Evaluate the tuned model\n",
        "evaluate_model(y_val, y_val_pred_tuned, y_val_pred_proba_tuned, model_name=\"Tuned Logistic Regression\")\n"
      ],
      "id": "cb56aa1f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4330546e"
      },
      "outputs": [],
      "source": [
        "# Compare baseline vs tuned logistic regression\n",
        "print(\"=\"*70)\n",
        "print(\"COMPARISON: Baseline vs Tuned Logistic Regression\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Calculate metrics for both\n",
        "baseline_auc = roc_auc_score(y_val, y_val_pred_proba)\n",
        "tuned_auc = roc_auc_score(y_val, y_val_pred_proba_tuned)\n",
        "\n",
        "baseline_recall = recall_score(y_val, y_val_pred, pos_label=1)\n",
        "tuned_recall = recall_score(y_val, y_val_pred_tuned, pos_label=1)\n",
        "\n",
        "baseline_f1 = f1_score(y_val, y_val_pred, pos_label=1)\n",
        "tuned_f1 = f1_score(y_val, y_val_pred_tuned, pos_label=1)\n",
        "\n",
        "comparison = pd.DataFrame({\n",
        "    'Metric': ['ROC-AUC', 'Recall', 'F1 Score'],\n",
        "    'Baseline LR': [baseline_auc, baseline_recall, baseline_f1],\n",
        "    'Tuned LR': [tuned_auc, tuned_recall, tuned_f1],\n",
        "    'Improvement': [tuned_auc - baseline_auc, tuned_recall - baseline_recall, tuned_f1 - baseline_f1]\n",
        "})\n",
        "\n",
        "print(comparison.to_string(index=False))\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Plot ROC curves side by side\n",
        "models_dict = {\n",
        "    'Baseline LR': y_val_pred_proba,\n",
        "    'Tuned LR': y_val_pred_proba_tuned\n",
        "}\n",
        "plot_multiple_roc_curves(models_dict, y_val)\n"
      ],
      "id": "4330546e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "877e8b91"
      },
      "outputs": [],
      "source": [
        "# Store results for later comparison with other models\n",
        "lr_results = {\n",
        "    'model_name': 'Logistic Regression (Tuned)',\n",
        "    'model': tuned_lr_model,\n",
        "    'y_pred': y_val_pred_tuned,\n",
        "    'y_pred_proba': y_val_pred_proba_tuned,\n",
        "    'roc_auc': tuned_auc,\n",
        "    'recall': tuned_recall,\n",
        "    'f1': tuned_f1,\n",
        "    'best_params': grid_search.best_params_\n",
        "}\n",
        "\n",
        "all_results['Logistic Regression (Tuned)'] = lr_results\n",
        "print(\"Saved results for LR (Tuned)\")\n"
      ],
      "id": "877e8b91"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "243e3ab9"
      },
      "source": [
        "### Decision Trees\n",
        "We will follow the same structure from above. We will use GridSearchCV to run cross validation as well as find optimal hyperparameters"
      ],
      "id": "243e3ab9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "686ad771"
      },
      "outputs": [],
      "source": [
        "# Init Pipeline\n",
        "dt_pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', DecisionTreeClassifier(random_state=42, class_weight='balanced'))\n",
        "])\n",
        "\n",
        "# Define Hyperparameter Grid\n",
        "param_grid = {\n",
        "    'classifier__max_depth': [2, 3, 4, 5, 7, 9, 11, 13, 15, None],\n",
        "    'classifier__min_samples_split': [2, 5, 10],\n",
        "    'classifier__min_samples_leaf': [1, 2, 4],\n",
        "    'classifier__criterion': ['gini', 'entropy']\n",
        "}"
      ],
      "id": "686ad771"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1058f2d2"
      },
      "outputs": [],
      "source": [
        "# Setup 5-fold cross validation (stratified to keep class balance in each fold)\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# GridSearch\n",
        "grid_dt = GridSearchCV(\n",
        "    estimator=dt_pipeline,\n",
        "    param_grid=param_grid,\n",
        "    cv=cv,\n",
        "    scoring='roc_auc',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit - this runs all the cross-validation\n",
        "grid_dt.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nGrid search complete!\")\n"
      ],
      "id": "1058f2d2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "160535b0"
      },
      "outputs": [],
      "source": [
        "print(\"Best params:\", grid_dt.best_params_)\n",
        "print(\"Best ROC-AUC:\", grid_dt.best_score_)"
      ],
      "id": "160535b0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bea90f9f"
      },
      "outputs": [],
      "source": [
        "# Let's look at all results\n",
        "results_df = pd.DataFrame(grid_dt.cv_results_)\n",
        "print(\"\\nAll cross-validation results:\")\n",
        "print(results_df[['param_classifier__max_depth', 'param_classifier__min_samples_split', 'param_classifier__min_samples_leaf' ,'param_classifier__min_samples_leaf','param_classifier__criterion', 'mean_test_score', 'std_test_score']].sort_values('mean_test_score', ascending=False))"
      ],
      "id": "bea90f9f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1b11502"
      },
      "outputs": [],
      "source": [
        "tuned_dt_model = grid_dt.best_estimator_\n",
        "\n",
        "# Make predictions on validation set\n",
        "y_val_pred_tuned_dt = tuned_dt_model.predict(X_val)\n",
        "y_val_pred_proba_tuned_dt = tuned_dt_model.predict_proba(X_val)[:, 1]\n",
        "\n",
        "# Evaluate the tuned model\n",
        "evaluate_model(y_val, y_val_pred_tuned_dt, y_val_pred_proba_tuned_dt, model_name=\"Tuned Decision Tree\")\n",
        "\n"
      ],
      "id": "c1b11502"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebf978cd"
      },
      "outputs": [],
      "source": [
        "# Compare baseline vs tuned Decision Tree\n",
        "print(\"=\"*70)\n",
        "print(\"COMPARISON: Baseline vs Tuned Decision Tree\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Calculate metrics\n",
        "tuned_dt_auc = roc_auc_score(y_val, y_val_pred_proba_tuned_dt)\n",
        "\n",
        "tuned_dt_recall = recall_score(y_val, y_val_pred_tuned_dt, pos_label=1)\n",
        "\n",
        "tuned_dt_f1 = f1_score(y_val, y_val_pred_tuned_dt, pos_label=1)\n",
        "\n",
        "comparison = pd.DataFrame({\n",
        "    'Metric': ['ROC-AUC', 'Recall', 'F1 Score'],\n",
        "    'Baseline  LR': [baseline_auc, baseline_recall, baseline_f1],\n",
        "    'Tuned DT': [tuned_dt_auc, tuned_dt_recall, tuned_dt_f1],\n",
        "    'Improvement': [tuned_dt_auc - baseline_auc, tuned_dt_recall - baseline_recall, tuned_dt_f1 - baseline_f1]\n",
        "})\n",
        "\n",
        "print(comparison.to_string(index=False))\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Plot ROC curves side by side\n",
        "models_dict = {\n",
        "    'Baseline LR': y_val_pred_proba,\n",
        "    'Tuned DT': y_val_pred_proba_tuned_dt\n",
        "}\n",
        "plot_multiple_roc_curves(models_dict, y_val)\n"
      ],
      "id": "ebf978cd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ee3756a0"
      },
      "outputs": [],
      "source": [
        "# Store results for later comparison with other models\n",
        "dt_results = {\n",
        "    'model_name': 'Decision Tree (Tuned)',\n",
        "    'model': tuned_dt_model,\n",
        "    'y_pred': y_val_pred_tuned_dt,\n",
        "    'y_pred_proba': y_val_pred_proba_tuned_dt,\n",
        "    'roc_auc': tuned_dt_auc,\n",
        "    'recall': tuned_dt_recall,\n",
        "    'f1': tuned_dt_f1,\n",
        "    'best_params': grid_dt.best_params_\n",
        "}\n",
        "\n",
        "all_results['Decision Tree (Tuned)'] = dt_results\n"
      ],
      "id": "ee3756a0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea3addff"
      },
      "source": [
        "### Let's Use Random Forests\n",
        "We will again follow the same process for RF."
      ],
      "id": "ea3addff"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "551dc79e"
      },
      "outputs": [],
      "source": [
        "# Init Pipeline and GridSearch Params\n",
        "rf_pipeline = Pipeline(steps=[\n",
        "    (\"preprocess\", preprocessor),   # same ColumnTransformer as before\n",
        "    (\"model\", RandomForestClassifier(\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    ))\n",
        "])\n",
        "\n",
        "param_grid_rf = {\n",
        "    \"model__n_estimators\": [100, 125, 250],      # number of trees\n",
        "    \"model__max_depth\": [None, 3, 4, 5, 7],         # tree depth\n",
        "    \"model__min_samples_split\": [2, 5, 10],      # min samples to split\n",
        "    \"model__min_samples_leaf\": [5, 8, 10, 11],        # min samples per leaf\n",
        "    \"model__max_features\": [\"sqrt\", \"log2\"]      # how many features to consider at each split\n",
        "}\n",
        "\n",
        "# Setup 5-fold cross validation (stratified to keep class balance in each fold)\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "rf_grid = GridSearchCV(\n",
        "    estimator=rf_pipeline,\n",
        "    param_grid=param_grid_rf,\n",
        "    scoring=\"roc_auc\",\n",
        "    cv=5,\n",
        "    n_jobs=-1\n",
        ")\n"
      ],
      "id": "551dc79e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0764d511"
      },
      "outputs": [],
      "source": [
        "rf_grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best RF params:\", rf_grid.best_params_)\n",
        "print(\"Best RF CV ROC-AUC:\", rf_grid.best_score_)"
      ],
      "id": "0764d511"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d69b51f7"
      },
      "outputs": [],
      "source": [
        "results_df = pd.DataFrame(rf_grid.cv_results_)\n",
        "\n",
        "best_rf_model = rf_grid.best_estimator_\n",
        "\n",
        "# Make predictions on validation set\n",
        "y_val_pred_tuned_rf = best_rf_model.predict(X_val)\n",
        "y_val_pred_proba_tuned_rf = best_rf_model.predict_proba(X_val)[:, 1]\n",
        "\n",
        "# Evaluate the tuned model\n",
        "evaluate_model(y_val, y_val_pred_tuned_rf, y_val_pred_proba_tuned_rf, model_name=\"Tuned Random Forest\")"
      ],
      "id": "d69b51f7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd59a30e"
      },
      "source": [
        "#### Threshold Tuning for Random Forest\n",
        "\n",
        "Since the default threshold (0.5) resulted in poor recall, let's tune the classification threshold to improve our ability to catch deaths.\n"
      ],
      "id": "bd59a30e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06b829e4"
      },
      "outputs": [],
      "source": [
        "# Test different thresholds\n",
        "thresholds = np.linspace(0.1, 0.9, 17)\n",
        "\n",
        "print(\"Random Forest - Threshold Tuning Results:\")\n",
        "print(\"=\"*70)\n",
        "threshold_results_rf = []\n",
        "\n",
        "for t in thresholds:\n",
        "    y_pred_t = (y_val_pred_proba_tuned_rf >= t).astype(int)\n",
        "    recall_t = recall_score(y_val, y_pred_t, pos_label=1, zero_division=0)\n",
        "    precision_t = precision_score(y_val, y_pred_t, pos_label=1, zero_division=0)\n",
        "    f1_t = f1_score(y_val, y_pred_t, pos_label=1, zero_division=0)\n",
        "\n",
        "    threshold_results_rf.append({\n",
        "        'threshold': t,\n",
        "        'recall': recall_t,\n",
        "        'precision': precision_t,\n",
        "        'f1': f1_t\n",
        "    })\n",
        "\n",
        "    print(f\"t={t:.2f} | Recall={recall_t:.3f} | Precision={precision_t:.3f} | F1={f1_t:.3f}\")\n",
        "\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Find best threshold based on F1 score\n",
        "threshold_df_rf = pd.DataFrame(threshold_results_rf)\n",
        "best_threshold_rf = threshold_df_rf.loc[threshold_df_rf['f1'].idxmax()]\n",
        "\n",
        "print(f\"\\nBest threshold for Random Forest: {best_threshold_rf['threshold']:.3f}\")\n",
        "print(f\"  Recall: {best_threshold_rf['recall']:.3f}\")\n",
        "print(f\"  Precision: {best_threshold_rf['precision']:.3f}\")\n",
        "print(f\"  F1 Score: {best_threshold_rf['f1']:.3f}\")\n",
        "\n",
        "# Apply best threshold\n",
        "y_pred_03 = (y_val_pred_proba_tuned_rf >= best_threshold_rf['threshold']).astype(int)\n",
        "\n",
        "print(f\"\\nApplied threshold: {best_threshold_rf['threshold']:.3f}\")"
      ],
      "id": "06b829e4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ccf1b0e"
      },
      "outputs": [],
      "source": [
        "# Compare baseline vs tuned Random Forest\n",
        "print(\"=\"*70)\n",
        "print(\"COMPARISON: Baseline vs Tuned Random Forest (0.5)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Calculate metrics\n",
        "tuned_rf_auc = roc_auc_score(y_val, y_val_pred_proba_tuned_rf)\n",
        "tuned_rf_recall = recall_score(y_val, y_val_pred_tuned_rf, pos_label=1)\n",
        "tuned_rf_f1 = f1_score(y_val, y_val_pred_tuned_rf, pos_label=1)\n",
        "\n",
        "comparison = pd.DataFrame({\n",
        "    'Metric': ['ROC-AUC', 'Recall', 'F1 Score'],\n",
        "    'Baseline  LR': [baseline_auc, baseline_recall, baseline_f1],\n",
        "    'Tuned DT': [tuned_rf_auc, tuned_rf_recall, tuned_rf_f1],\n",
        "    'Improvement': [tuned_rf_auc - baseline_auc, tuned_rf_recall - baseline_recall, tuned_rf_f1 - baseline_f1]\n",
        "})\n",
        "\n",
        "print(comparison.to_string(index=False))\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Plot ROC curves side by side\n",
        "models_dict = {\n",
        "    'Baseline LR': y_val_pred_proba,\n",
        "    'Tuned DT': y_val_pred_proba_tuned_rf\n",
        "}\n",
        "plot_multiple_roc_curves(models_dict, y_val)\n"
      ],
      "id": "3ccf1b0e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5647d746"
      },
      "outputs": [],
      "source": [
        "# Store results for later comparison with other models\n",
        "rf_results = {\n",
        "    'model_name': 'Random Forest (Tuned)',\n",
        "    'model': best_rf_model,\n",
        "    'y_pred': y_val_pred_tuned_rf,\n",
        "    'y_pred_proba': y_val_pred_proba_tuned_rf,\n",
        "    'roc_auc': tuned_rf_auc,\n",
        "    'recall': tuned_rf_recall,\n",
        "    'f1': tuned_rf_f1,\n",
        "    'best_params': rf_grid.best_params_\n",
        "}\n",
        "\n",
        "all_results['Random Forest (Tuned) (0.5)'] = rf_results"
      ],
      "id": "5647d746"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bd1b9598"
      },
      "outputs": [],
      "source": [
        "# Compare baseline vs tuned Random Forest\n",
        "print(\"=\"*70)\n",
        "print(\"COMPARISON: Baseline vs Tuned Random Forest (0.225)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Calculate metrics\n",
        "tuned_rf_auc = roc_auc_score(y_val, y_pred_03)\n",
        "tuned_rf_recall = recall_score(y_val, y_pred_03, pos_label=1)\n",
        "tuned_rf_f1 = f1_score(y_val, y_pred_03, pos_label=1)\n",
        "\n",
        "comparison = pd.DataFrame({\n",
        "    'Metric': ['ROC-AUC', 'Recall', 'F1 Score'],\n",
        "    'Baseline  LR': [baseline_auc, baseline_recall, baseline_f1],\n",
        "    'Tuned DT': [tuned_rf_auc, tuned_rf_recall, tuned_rf_f1],\n",
        "    'Improvement': [tuned_rf_auc - baseline_auc, tuned_rf_recall - baseline_recall, tuned_rf_f1 - baseline_f1]\n",
        "})\n",
        "\n",
        "print(comparison.to_string(index=False))\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Plot ROC curves side by side\n",
        "models_dict = {\n",
        "    'Baseline LR': y_val_pred_proba,\n",
        "    'Tuned DT': y_pred_03\n",
        "}\n",
        "plot_multiple_roc_curves(models_dict, y_val)\n"
      ],
      "id": "bd1b9598"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bce22245"
      },
      "outputs": [],
      "source": [
        "# Update results with tuned threshold\n",
        "rf_results_tuned = {\n",
        "    'model_name': f\"Random Forest (Tuned, t={best_threshold_rf['threshold']:.3f})\",\n",
        "    'model': best_rf_model,\n",
        "    'y_pred': y_pred_03,\n",
        "    'y_pred_proba': y_val_pred_proba_tuned_rf,\n",
        "    'roc_auc': roc_auc_score(y_val, y_val_pred_proba_tuned_rf),\n",
        "    'recall': best_threshold_rf['recall'],\n",
        "    'f1': best_threshold_rf['f1'],\n",
        "    'threshold': best_threshold_rf['threshold'],\n",
        "    'best_params': rf_grid.best_params_\n",
        "}\n",
        "\n",
        "all_results[f\"Random Forest (Tuned, t={best_threshold_rf['threshold']:.3f})\"] = rf_results_tuned\n",
        "print(f\"\\nSaved tuned threshold results for Random Forest!\")"
      ],
      "id": "bce22245"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e60505a9"
      },
      "outputs": [],
      "source": [
        "# Visualize threshold tuning for Random Forest\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(threshold_df_rf['threshold'], threshold_df_rf['recall'], 'o-', label='Recall', linewidth=2, markersize=6)\n",
        "plt.plot(threshold_df_rf['threshold'], threshold_df_rf['precision'], 's-', label='Precision', linewidth=2, markersize=6)\n",
        "plt.plot(threshold_df_rf['threshold'], threshold_df_rf['f1'], '^-', label='F1 Score', linewidth=2, markersize=6)\n",
        "plt.axvline(x=best_threshold_rf['threshold'], color='red', linestyle='--', linewidth=2, label=f\"Best t={best_threshold_rf['threshold']:.3f}\")\n",
        "plt.axvline(x=0.5, color='gray', linestyle=':', linewidth=1, alpha=0.7, label='Default t=0.5')\n",
        "plt.xlabel('Threshold', fontsize=12)\n",
        "plt.ylabel('Score', fontsize=12)\n",
        "plt.title('Random Forest: Threshold Tuning', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc='best')\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "id": "e60505a9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "110cda31"
      },
      "outputs": [],
      "source": [
        "# Show impact of threshold tuning for Random Forest\n",
        "print(\"=\"*80)\n",
        "print(\"IMPACT OF THRESHOLD TUNING - Random Forest\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Get default (t=0.5) metrics\n",
        "default_rf_recall = recall_score(y_val, y_val_pred_tuned_rf, pos_label=1)\n",
        "default_rf_f1 = f1_score(y_val, y_val_pred_tuned_rf, pos_label=1)\n",
        "\n",
        "print(f\"  Default (t=0.5):  Recall={default_rf_recall:.3f}, F1={default_rf_f1:.3f}\")\n",
        "print(f\"  Tuned (t={best_threshold_rf['threshold']:.3f}):    Recall={best_threshold_rf['recall']:.3f}, F1={best_threshold_rf['f1']:.3f}\")\n",
        "print(f\"  Improvement:      Recall=+{best_threshold_rf['recall'] - default_rf_recall:.3f}, F1=+{best_threshold_rf['f1'] - default_rf_f1:.3f}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Print confusion matrices for default and tuned thresholds\n",
        "print(\"Confusion Matrix (Default t=0.5):\")\n",
        "print(confusion_matrix(y_val, y_val_pred_tuned_rf))\n",
        "print(\"\\nConfusion Matrix (Tuned t={:.3f}):\".format(best_threshold_rf['threshold']))\n",
        "print(confusion_matrix(y_val, y_pred_03))\n"
      ],
      "id": "110cda31"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "640d6db8"
      },
      "source": [
        "### Gradient Boosting/XGBoost\n",
        "We will follow the same process for Gradient Boosting and XGBoost"
      ],
      "id": "640d6db8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64eaa521"
      },
      "source": [
        "### Gradient Boosting Classifier\n",
        "\n",
        "Gradient Boosting builds an ensemble of weak learners (typically shallow trees) sequentially, where each new tree tries to correct the errors of the previous ones. It's known for strong performance on tabular data.\n"
      ],
      "id": "64eaa521"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81492b17"
      },
      "outputs": [],
      "source": [
        "# Gradient Boosting Pipeline\n",
        "gb_pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', GradientBoostingClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "# Hyperparameter grid - tuning key GB parameters\n",
        "param_grid_gb = {\n",
        "    'classifier__n_estimators': [50, 100, 200],  # Number of boosting stages\n",
        "    'classifier__learning_rate': [0.01, 0.1, 0.2],  # Shrinks contribution of each tree\n",
        "    'classifier__max_depth': [3, 4, 5],  # Max depth of individual trees\n",
        "    'classifier__min_samples_split': [2, 5, 10],  # Min samples to split node\n",
        "    'classifier__subsample': [0.8, 1.0]  # Fraction of samples for fitting trees\n",
        "}\n",
        "\n",
        "print(f\"Testing {len(param_grid_gb['classifier__n_estimators']) * len(param_grid_gb['classifier__learning_rate']) * len(param_grid_gb['classifier__max_depth']) * len(param_grid_gb['classifier__min_samples_split']) * len(param_grid_gb['classifier__subsample'])} combinations\")\n",
        "\n",
        "# GridSearchCV\n",
        "grid_gb = GridSearchCV(\n",
        "    estimator=gb_pipeline,\n",
        "    param_grid=param_grid_gb,\n",
        "    cv=cv,\n",
        "    scoring='roc_auc',\n",
        "    n_jobs=-1,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "# Fit\n",
        "grid_gb.fit(X_train, y_train)\n",
        "print(\"\\nGradient Boosting grid search complete!\")\n"
      ],
      "id": "81492b17"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fab61e45"
      },
      "outputs": [],
      "source": [
        "# Best parameters and score\n",
        "print(\"Best parameters found:\")\n",
        "print(grid_gb.best_params_)\n",
        "print(f\"\\nBest cross-validated ROC-AUC score: {grid_gb.best_score_:.4f}\")\n",
        "\n",
        "# Show top 5 configurations\n",
        "results_df_gb = pd.DataFrame(grid_gb.cv_results_)\n",
        "top_results = results_df_gb[['params', 'mean_test_score', 'std_test_score']].sort_values('mean_test_score', ascending=False).head(5)\n",
        "print(\"\\nTop 5 hyperparameter configurations:\")\n",
        "for idx, row in top_results.iterrows():\n",
        "    print(f\"Score: {row['mean_test_score']:.4f} (+/- {row['std_test_score']:.4f}) - {row['params']}\")\n"
      ],
      "id": "fab61e45"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "851c35ac"
      },
      "outputs": [],
      "source": [
        "# Get best model and make predictions\n",
        "tuned_gb_model = grid_gb.best_estimator_\n",
        "\n",
        "y_val_pred_gb = tuned_gb_model.predict(X_val)\n",
        "y_val_pred_proba_gb = tuned_gb_model.predict_proba(X_val)[:, 1]\n",
        "\n",
        "print(\"Tuned Gradient Boosting model ready for evaluation!\")\n"
      ],
      "id": "851c35ac"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3770a8d"
      },
      "outputs": [],
      "source": [
        "# Evaluate Gradient Boosting model\n",
        "evaluate_model(y_val, y_val_pred_gb, y_val_pred_proba_gb, model_name=\"Tuned Gradient Boosting\")\n"
      ],
      "id": "d3770a8d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8e6e39dc"
      },
      "outputs": [],
      "source": [
        "# Store results\n",
        "gb_auc = roc_auc_score(y_val, y_val_pred_proba_gb)\n",
        "gb_recall = recall_score(y_val, y_val_pred_gb, pos_label=1)\n",
        "gb_f1 = f1_score(y_val, y_val_pred_gb, pos_label=1)\n",
        "\n",
        "gb_results = {\n",
        "    'model_name': 'Gradient Boosting (Tuned)',\n",
        "    'model': tuned_gb_model,\n",
        "    'y_pred': y_val_pred_gb,\n",
        "    'y_pred_proba': y_val_pred_proba_gb,\n",
        "    'roc_auc': gb_auc,\n",
        "    'recall': gb_recall,\n",
        "    'f1': gb_f1,\n",
        "    'best_params': grid_gb.best_params_\n",
        "}\n",
        "\n",
        "all_results['Gradient Boosting (Tuned)'] = gb_results\n",
        "print(\"Saved results for Gradient Boosting!\")\n"
      ],
      "id": "8e6e39dc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "246d1831"
      },
      "source": [
        "#### Threshold Tuning for Gradient Boosting"
      ],
      "id": "246d1831"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fc40df37"
      },
      "outputs": [],
      "source": [
        "# Test different thresholds\n",
        "thresholds = np.linspace(0.1, 0.9, 17)\n",
        "\n",
        "print(\"Gradient Boosting - Threshold Tuning Results:\")\n",
        "print(\"=\"*70)\n",
        "threshold_results_gb = []\n",
        "\n",
        "for t in thresholds:\n",
        "    y_pred_t = (y_val_pred_proba_gb >= t).astype(int)\n",
        "    recall_t = recall_score(y_val, y_pred_t, pos_label=1, zero_division=0)\n",
        "    precision_t = precision_score(y_val, y_pred_t, pos_label=1, zero_division=0)\n",
        "    f1_t = f1_score(y_val, y_pred_t, pos_label=1, zero_division=0)\n",
        "\n",
        "    threshold_results_gb.append({\n",
        "        'threshold': t,\n",
        "        'recall': recall_t,\n",
        "        'precision': precision_t,\n",
        "        'f1': f1_t\n",
        "    })\n",
        "\n",
        "    print(f\"t={t:.2f} | Recall={recall_t:.3f} | Precision={precision_t:.3f} | F1={f1_t:.3f}\")\n",
        "\n",
        "print(\"=\"*70)\n"
      ],
      "id": "fc40df37"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60c1fb32"
      },
      "outputs": [],
      "source": [
        "# Find best threshold based on F1 score (balances recall and precision)\n",
        "threshold_df_gb = pd.DataFrame(threshold_results_gb)\n",
        "best_threshold_gb = threshold_df_gb.loc[threshold_df_gb['f1'].idxmax()]\n",
        "\n",
        "print(f\"\\nBest threshold for Gradient Boosting: {best_threshold_gb['threshold']:.3f}\")\n",
        "print(f\"  Recall: {best_threshold_gb['recall']:.3f}\")\n",
        "print(f\"  Precision: {best_threshold_gb['precision']:.3f}\")\n",
        "print(f\"  F1 Score: {best_threshold_gb['f1']:.3f}\")\n",
        "\n",
        "# Apply best threshold\n",
        "y_val_pred_gb_tuned = (y_val_pred_proba_gb >= best_threshold_gb['threshold']).astype(int)\n",
        "\n",
        "# Update results with tuned threshold\n",
        "gb_results_tuned = {\n",
        "    'model_name': f\"Gradient Boosting (Tuned, t={best_threshold_gb['threshold']:.3f})\",\n",
        "    'model': tuned_gb_model,\n",
        "    'y_pred': y_val_pred_gb_tuned,\n",
        "    'y_pred_proba': y_val_pred_proba_gb,\n",
        "    'roc_auc': roc_auc_score(y_val, y_val_pred_proba_gb),\n",
        "    'recall': best_threshold_gb['recall'],\n",
        "    'f1': best_threshold_gb['f1'],\n",
        "    'threshold': best_threshold_gb['threshold'],\n",
        "    'best_params': grid_gb.best_params_\n",
        "}\n",
        "\n",
        "all_results[f\"Gradient Boosting (Tuned, t={best_threshold_gb['threshold']:.3f})\"] = gb_results_tuned\n",
        "print(f\"\\nSaved tuned threshold results for Gradient Boosting!\")\n"
      ],
      "id": "60c1fb32"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b67ccd83"
      },
      "source": [
        "### XGBoost Classifier\n",
        "\n",
        "XGBoost (Extreme Gradient Boosting) is an optimized implementation of gradient boosting that's faster and often performs better. It includes regularization and handles missing values natively.\n"
      ],
      "id": "b67ccd83"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2967889"
      },
      "outputs": [],
      "source": [
        "# XGBoost Pipeline\n",
        "xgb_pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', XGBClassifier(random_state=42, eval_metric='logloss'))\n",
        "])\n",
        "\n",
        "# Hyperparameter grid - tuning key XGBoost parameters\n",
        "param_grid_xgb = {\n",
        "    'classifier__n_estimators': [50, 100, 150, 175, 200],  # Number of boosting rounds\n",
        "    'classifier__learning_rate': [0.001, 0.01, 0.1, 0.3],  # Step size shrinkage\n",
        "    'classifier__max_depth': [3, 5, 7],  # Max tree depth\n",
        "    'classifier__min_child_weight': [1, 3, 5],  # Min sum of instance weight in child\n",
        "    'classifier__subsample': [0.8, 1.0],  # Subsample ratio of training instances\n",
        "    'classifier__colsample_bytree': [0.8, 1.0]  # Subsample ratio of columns\n",
        "}\n",
        "\n",
        "print(f\"Testing {len(param_grid_xgb['classifier__n_estimators']) * len(param_grid_xgb['classifier__learning_rate']) * len(param_grid_xgb['classifier__max_depth']) * len(param_grid_xgb['classifier__min_child_weight']) * len(param_grid_xgb['classifier__subsample']) * len(param_grid_xgb['classifier__colsample_bytree'])} combinations\")\n",
        "\n",
        "# GridSearchCV\n",
        "grid_xgb = GridSearchCV(\n",
        "    estimator=xgb_pipeline,\n",
        "    param_grid=param_grid_xgb,\n",
        "    cv=cv,\n",
        "    scoring='roc_auc',\n",
        "    n_jobs=-1,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "# Fit\n",
        "grid_xgb.fit(X_train, y_train)\n",
        "print(\"\\nXGBoost grid search complete!\")\n"
      ],
      "id": "e2967889"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8e016453"
      },
      "outputs": [],
      "source": [
        "# Best parameters and score\n",
        "print(\"Best parameters found:\")\n",
        "print(grid_xgb.best_params_)\n",
        "print(f\"\\nBest cross-validated ROC-AUC score: {grid_xgb.best_score_:.4f}\")\n",
        "\n",
        "# Show top 5 configurations\n",
        "results_df_xgb = pd.DataFrame(grid_xgb.cv_results_)\n",
        "top_results = results_df_xgb[['params', 'mean_test_score', 'std_test_score']].sort_values('mean_test_score', ascending=False).head(5)\n",
        "print(\"\\nTop 5 hyperparameter configurations:\")\n",
        "for idx, row in top_results.iterrows():\n",
        "    print(f\"Score: {row['mean_test_score']:.4f} (+/- {row['std_test_score']:.4f}) - {row['params']}\")\n"
      ],
      "id": "8e016453"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "030884f3"
      },
      "outputs": [],
      "source": [
        "# Get best model and make predictions\n",
        "tuned_xgb_model = grid_xgb.best_estimator_\n",
        "\n",
        "y_val_pred_xgb = tuned_xgb_model.predict(X_val)\n",
        "y_val_pred_proba_xgb = tuned_xgb_model.predict_proba(X_val)[:, 1]\n",
        "\n",
        "print(\"Tuned XGBoost model ready for evaluation!\")\n"
      ],
      "id": "030884f3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ac886a2"
      },
      "outputs": [],
      "source": [
        "# Evaluate XGBoost model\n",
        "evaluate_model(y_val, y_val_pred_xgb, y_val_pred_proba_xgb, model_name=\"Tuned XGBoost\")\n"
      ],
      "id": "6ac886a2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b54816c4"
      },
      "outputs": [],
      "source": [
        "# Store results\n",
        "xgb_auc = roc_auc_score(y_val, y_val_pred_proba_xgb)\n",
        "xgb_recall = recall_score(y_val, y_val_pred_xgb, pos_label=1)\n",
        "xgb_f1 = f1_score(y_val, y_val_pred_xgb, pos_label=1)\n",
        "\n",
        "xgb_results = {\n",
        "    'model_name': 'XGBoost (Tuned)',\n",
        "    'model': tuned_xgb_model,\n",
        "    'y_pred': y_val_pred_xgb,\n",
        "    'y_pred_proba': y_val_pred_proba_xgb,\n",
        "    'roc_auc': xgb_auc,\n",
        "    'recall': xgb_recall,\n",
        "    'f1': xgb_f1,\n",
        "    'best_params': grid_xgb.best_params_\n",
        "}\n",
        "\n",
        "all_results['XGBoost (Tuned)'] = xgb_results\n",
        "print(\"Saved results for XGBoost!\")\n"
      ],
      "id": "b54816c4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b5b7d2d"
      },
      "source": [
        "#### Threshold Tuning for XGBoost\n"
      ],
      "id": "6b5b7d2d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcb88f8f"
      },
      "outputs": [],
      "source": [
        "# Test different thresholds\n",
        "thresholds = np.linspace(0.1, 0.9, 17)\n",
        "\n",
        "print(\"XGBoost - Threshold Tuning Results:\")\n",
        "print(\"=\"*70)\n",
        "threshold_results_xgb = []\n",
        "\n",
        "for t in thresholds:\n",
        "    y_pred_t = (y_val_pred_proba_xgb >= t).astype(int)\n",
        "    recall_t = recall_score(y_val, y_pred_t, pos_label=1, zero_division=0)\n",
        "    precision_t = precision_score(y_val, y_pred_t, pos_label=1, zero_division=0)\n",
        "    f1_t = f1_score(y_val, y_pred_t, pos_label=1, zero_division=0)\n",
        "\n",
        "    threshold_results_xgb.append({\n",
        "        'threshold': t,\n",
        "        'recall': recall_t,\n",
        "        'precision': precision_t,\n",
        "        'f1': f1_t\n",
        "    })\n",
        "\n",
        "    print(f\"t={t:.2f} | Recall={recall_t:.3f} | Precision={precision_t:.3f} | F1={f1_t:.3f}\")\n",
        "\n",
        "print(\"=\"*70)\n"
      ],
      "id": "bcb88f8f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9e0fc657"
      },
      "outputs": [],
      "source": [
        "# Find best threshold based on F1 score (balances recall and precision)\n",
        "threshold_df_xgb = pd.DataFrame(threshold_results_xgb)\n",
        "best_threshold_xgb = threshold_df_xgb.loc[threshold_df_xgb['f1'].idxmax()]\n",
        "\n",
        "print(f\"\\nBest threshold for XGBoost: {best_threshold_xgb['threshold']:.3f}\")\n",
        "print(f\"  Recall: {best_threshold_xgb['recall']:.3f}\")\n",
        "print(f\"  Precision: {best_threshold_xgb['precision']:.3f}\")\n",
        "print(f\"  F1 Score: {best_threshold_xgb['f1']:.3f}\")\n",
        "\n",
        "# Apply best threshold\n",
        "y_val_pred_xgb_tuned = (y_val_pred_proba_xgb >= best_threshold_xgb['threshold']).astype(int)\n",
        "\n",
        "# Update results with tuned threshold\n",
        "xgb_results_tuned = {\n",
        "    'model_name': f\"XGBoost (Tuned, t={best_threshold_xgb['threshold']:.3f})\",\n",
        "    'model': tuned_xgb_model,\n",
        "    'y_pred': y_val_pred_xgb_tuned,\n",
        "    'y_pred_proba': y_val_pred_proba_xgb,\n",
        "    'roc_auc': roc_auc_score(y_val, y_val_pred_proba_xgb),\n",
        "    'recall': best_threshold_xgb['recall'],\n",
        "    'f1': best_threshold_xgb['f1'],\n",
        "    'threshold': best_threshold_xgb['threshold'],\n",
        "    'best_params': grid_xgb.best_params_\n",
        "}\n",
        "\n",
        "all_results[f\"XGBoost (Tuned, t={best_threshold_xgb['threshold']:.3f})\"] = xgb_results_tuned\n",
        "print(f\"\\nSaved tuned threshold results for XGBoost!\")\n"
      ],
      "id": "9e0fc657"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2118ed3d"
      },
      "source": [
        "#### Impact of Threshold Tuning\n",
        "\n",
        "Let's visualize how threshold tuning affected the performance of both models.\n"
      ],
      "id": "2118ed3d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33b1c2ac"
      },
      "outputs": [],
      "source": [
        "# Compare before and after threshold tuning\n",
        "print(\"=\"*80)\n",
        "print(\"IMPACT OF THRESHOLD TUNING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Gradient Boosting comparison\n",
        "print(\"\\nGradient Boosting:\")\n",
        "print(f\"  Default (t=0.5):  Recall={gb_recall:.3f}, F1={gb_f1:.3f}\")\n",
        "print(f\"  Tuned (t={best_threshold_gb['threshold']:.3f}):    Recall={best_threshold_gb['recall']:.3f}, F1={best_threshold_gb['f1']:.3f}\")\n",
        "print(f\"  Improvement:      Recall=+{best_threshold_gb['recall'] - gb_recall:.3f}, F1=+{best_threshold_gb['f1'] - gb_f1:.3f}\")\n",
        "\n",
        "# XGBoost comparison\n",
        "print(\"\\nXGBoost:\")\n",
        "print(f\"  Default (t=0.5):  Recall={xgb_recall:.3f}, F1={xgb_f1:.3f}\")\n",
        "print(f\"  Tuned (t={best_threshold_xgb['threshold']:.3f}):    Recall={best_threshold_xgb['recall']:.3f}, F1={best_threshold_xgb['f1']:.3f}\")\n",
        "print(f\"  Improvement:      Recall=+{best_threshold_xgb['recall'] - xgb_recall:.3f}, F1=+{best_threshold_xgb['f1'] - xgb_f1:.3f}\")\n",
        "\n",
        "print(\"=\"*80)\n"
      ],
      "id": "33b1c2ac"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0f62771"
      },
      "outputs": [],
      "source": [
        "# Plot threshold tuning curves\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Gradient Boosting\n",
        "axes[0].plot(threshold_df_gb['threshold'], threshold_df_gb['recall'], 'o-', label='Recall', linewidth=2, markersize=6)\n",
        "axes[0].plot(threshold_df_gb['threshold'], threshold_df_gb['precision'], 's-', label='Precision', linewidth=2, markersize=6)\n",
        "axes[0].plot(threshold_df_gb['threshold'], threshold_df_gb['f1'], '^-', label='F1 Score', linewidth=2, markersize=6)\n",
        "axes[0].axvline(x=best_threshold_gb['threshold'], color='red', linestyle='--', linewidth=2, label=f\"Best t={best_threshold_gb['threshold']:.3f}\")\n",
        "axes[0].axvline(x=0.5, color='gray', linestyle=':', linewidth=1, alpha=0.7, label='Default t=0.5')\n",
        "axes[0].set_xlabel('Threshold', fontsize=12)\n",
        "axes[0].set_ylabel('Score', fontsize=12)\n",
        "axes[0].set_title('Gradient Boosting: Threshold Tuning', fontsize=14, fontweight='bold')\n",
        "axes[0].legend(loc='best')\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "# XGBoost\n",
        "axes[1].plot(threshold_df_xgb['threshold'], threshold_df_xgb['recall'], 'o-', label='Recall', linewidth=2, markersize=6)\n",
        "axes[1].plot(threshold_df_xgb['threshold'], threshold_df_xgb['precision'], 's-', label='Precision', linewidth=2, markersize=6)\n",
        "axes[1].plot(threshold_df_xgb['threshold'], threshold_df_xgb['f1'], '^-', label='F1 Score', linewidth=2, markersize=6)\n",
        "axes[1].axvline(x=best_threshold_xgb['threshold'], color='red', linestyle='--', linewidth=2, label=f\"Best t={best_threshold_xgb['threshold']:.3f}\")\n",
        "axes[1].axvline(x=0.5, color='gray', linestyle=':', linewidth=1, alpha=0.7, label='Default t=0.5')\n",
        "axes[1].set_xlabel('Threshold', fontsize=12)\n",
        "axes[1].set_ylabel('Score', fontsize=12)\n",
        "axes[1].set_title('XGBoost: Threshold Tuning', fontsize=14, fontweight='bold')\n",
        "axes[1].legend(loc='best')\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "id": "b0f62771"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b81d87f7"
      },
      "source": [
        "### Final Model Comparison\n",
        "\n",
        "Now let's compare all the models we've trained to see which performs best overall.\n"
      ],
      "id": "b81d87f7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "df8127dc"
      },
      "outputs": [],
      "source": [
        "# Create comprehensive comparison table\n",
        "comparison_data = []\n",
        "for model_name, results in all_results.items():\n",
        "    comparison_data.append({\n",
        "        'Model': model_name,\n",
        "        'ROC-AUC': results['roc_auc'],\n",
        "        'Recall': results['recall'],\n",
        "        'F1 Score': results['f1']\n",
        "    })\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "comparison_df = comparison_df.sort_values('ROC-AUC', ascending=False)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"FINAL MODEL COMPARISON - All Tuned Models\")\n",
        "print(\"=\"*80)\n",
        "print(comparison_df.to_string(index=False))\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Highlight best model for each metric\n",
        "print(\"\\nBest Models by Metric:\")\n",
        "print(f\"  Highest ROC-AUC: {comparison_df.iloc[0]['Model']} ({comparison_df.iloc[0]['ROC-AUC']:.4f})\")\n",
        "print(f\"  Highest Recall:  {comparison_df.loc[comparison_df['Recall'].idxmax()]['Model']} ({comparison_df['Recall'].max():.4f})\")\n",
        "print(f\"  Highest F1:      {comparison_df.loc[comparison_df['F1 Score'].idxmax()]['Model']} ({comparison_df['F1 Score'].max():.4f})\")\n",
        "print(\"=\"*80)\n"
      ],
      "id": "df8127dc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83e4c1a8"
      },
      "outputs": [],
      "source": [
        "# Plot all ROC curves together\n",
        "all_models_roc = {}\n",
        "for model_name, results in all_results.items():\n",
        "    all_models_roc[model_name] = results['y_pred_proba']\n",
        "\n",
        "plot_multiple_roc_curves(all_models_roc, y_val)\n"
      ],
      "id": "83e4c1a8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd5b7f68"
      },
      "source": [
        "### Summary and Next Steps\n",
        "\n",
        "**Models Evaluated:**\n",
        "1. **Logistic Regression** - Interpretable linear baseline with L2 regularization\n",
        "2. **Decision Tree** - Non-linear, interpretable tree-based model\n",
        "3. **Random Forest** - Ensemble of trees, captures interactions\n",
        "4. **Gradient Boosting** - Sequential boosting, strong performance\n",
        "5. **XGBoost** - Optimized gradient boosting with regularization\n",
        "\n",
        "**Key Findings:**\n",
        "- All tuned models outperformed their baseline logistic regression\n",
        "- GridSearchCV with 5-fold stratified CV ensured robust hyperparameter selection\n",
        "- Class imbalance handled with `class_weight='balanced'` where applicable\n",
        "- ROC-AUC used as primary metric due to class imbalance\n",
        "\n",
        "**Clinical Considerations:**\n",
        "- **Recall** is critical - we want to catch as many deaths as possible\n",
        "- **ROC-AUC** provides best overall ranking of patient risk\n",
        "- **F1 Score** balances precision and recall for overall performance\n",
        "\n"
      ],
      "id": "cd5b7f68"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e090b258"
      },
      "source": [
        "Based on our findings, XGBoost (Tuned, t=0.250) is the best model as it has the best ROC-AUC performance as well as Recall and F1."
      ],
      "id": "e090b258"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be71f7cc"
      },
      "source": [
        "# Neural Networks with PyTorch\n",
        "This notebook will be used for nn as an educational extension and will compare its performance. For this dataset, since it is small and tabular, a tree-based model may be more appropriate.  \n",
        "\n",
        "This is just for experience and comparative analysis with notebook 03\n"
      ],
      "id": "be71f7cc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d463f49"
      },
      "source": [],
      "id": "9d463f49"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37070739"
      },
      "source": [
        "# Final Model Evaluation & Reporting\n",
        "\n",
        "This notebook contains the final evaluation, interpretation, and reporting for our heart failure mortality prediction model.\n",
        "\n",
        "**Note:**\n",
        "- **EDA** was completed in `01-EDA.ipynb`\n",
        "- **Data cleaning and preprocessing** were completed in `02-data-cleaning-preprocessing.ipynb`\n",
        "- **Model training, hyperparameter tuning, and threshold optimization** were completed in `03-ml-and-predictions.ipynb`\n",
        "\n",
        "This notebook focuses on:\n",
        "- Model selection and justification\n",
        "- Feature importance analysis and interpretation\n",
        "- Final evaluation on validation set (used as test set)\n",
        "- Model interpretation and explainability\n",
        "- Overfitting analysis\n"
      ],
      "id": "37070739"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bd3c1e75"
      },
      "outputs": [],
      "source": [
        "# Standard libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Preprocessing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# ML Models\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Evaluation metrics\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    roc_auc_score,\n",
        "    roc_curve,\n",
        "    precision_recall_curve,\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        "    ConfusionMatrixDisplay\n",
        ")\n",
        "\n",
        "# Feature importance\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "# Set plot style\n",
        "sns.set_style('darkgrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n"
      ],
      "id": "bd3c1e75"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2354902f"
      },
      "source": [
        "## Data Loading\n",
        "\n",
        "**Note:** Data cleaning, preprocessing, and train/validation splits were completed in `02-data-cleaning-preprocessing.ipynb`.\n",
        "\n",
        "We use the validation set from the previous notebooks as our final evaluation set (test set). This follows proper ML practice where the validation set serves as the held-out test set for final evaluation.\n"
      ],
      "id": "2354902f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "075c0b4c"
      },
      "outputs": [],
      "source": [
        "# Define feature lists (same as previous notebooks)\n",
        "continuous_features = ['age', 'creatinine_phosphokinase', 'ejection_fraction', 'platelets', 'serum_creatinine', 'serum_sodium', 'time']\n",
        "categorical_features = ['anaemia', 'diabetes', 'high_blood_pressure', 'sex', 'smoking']\n",
        "\n",
        "print(\"Data loaded from previous notebooks:\")\n",
        "print(f\"  Training set: {X_train.shape[0]} samples\")\n",
        "print(f\"  Validation set (used as test): {X_val.shape[0]} samples\")\n",
        "print(f\"  Training class distribution: {np.bincount(y_train)}\")\n",
        "print(f\"  Validation class distribution: {np.bincount(y_val)}\")\n"
      ],
      "id": "075c0b4c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d012f025"
      },
      "source": [
        "## Model Selection\n",
        "\n",
        "**Note:** Model comparison, hyperparameter tuning, and threshold optimization were completed in `03-ml-and-predictions.ipynb`.\n",
        "\n",
        "**Selected Primary Model: XGBoost (Tuned, t=0.250)**\n",
        "\n",
        "Based on cross-validation results, XGBoost with threshold tuning achieved:\n",
        "- Highest ROC-AUC: 0.944\n",
        "- High Recall: 0.917 (critical for catching deaths)\n",
        "- Best F1 Score: 0.815\n",
        "\n",
        "This model was selected because:\n",
        "1. **Best overall performance** - Highest ROC-AUC among all models\n",
        "2. **Strong recall** - Critical for clinical application (catching deaths)\n",
        "3. **Good balance** - F1 score shows good precision-recall trade-off\n",
        "4. **Robust** - Tree-based models handle non-linear relationships well\n",
        "5. **Interpretable** - Feature importance available for clinical understanding\n"
      ],
      "id": "d012f025"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17d5112f"
      },
      "source": [
        "## Model Loading\n",
        "\n",
        "**Note:** The XGBoost model was trained, tuned, and optimized in `03-ml-and-predictions.ipynb`. Here we recreate the preprocessing pipeline and model structure to extract feature importance and make predictions. The exact trained model from notebook 03 should be used for production, but we recreate it here for evaluation purposes.\n"
      ],
      "id": "17d5112f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dee315a"
      },
      "outputs": [],
      "source": [
        "# Create preprocessing pipeline (same as previous notebooks)\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), continuous_features),\n",
        "        ('cat', 'passthrough', categorical_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Create XGBoost pipeline with optimal parameters from notebook 03\n",
        "# Note: These are the best hyperparameters found via GridSearchCV in notebook 03\n",
        "xgb_pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', XGBClassifier(\n",
        "        random_state=42,\n",
        "        eval_metric='logloss',\n",
        "        n_estimators=150,\n",
        "        learning_rate=0.01,\n",
        "        max_depth=3,\n",
        "        min_child_weight=5,\n",
        "        subsample=1.0,\n",
        "        colsample_bytree=0.8\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Train on training set (for evaluation purposes - actual model trained in notebook 03)\n",
        "print(\"Training XGBoost model for evaluation (model was trained in notebook 03)...\")\n",
        "xgb_pipeline.fit(X_train, y_train)\n",
        "print(\"Model ready for evaluation!\")\n",
        "\n",
        "# Optimal threshold found in notebook 03\n",
        "optimal_threshold = 0.250\n",
        "print(f\"\\nUsing optimal threshold: {optimal_threshold} (determined in notebook 03)\")\n"
      ],
      "id": "9dee315a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "732156ee"
      },
      "source": [
        "## Feature Importance Analysis\n",
        "\n",
        "Understanding which features drive predictions is crucial for clinical interpretation. We'll analyze feature importance using multiple methods.\n"
      ],
      "id": "732156ee"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "054c51b1"
      },
      "outputs": [],
      "source": [
        "# Get feature names after preprocessing\n",
        "feature_names = continuous_features + categorical_features\n",
        "\n",
        "# Extract XGBoost model from pipeline\n",
        "xgb_model = xgb_pipeline.named_steps['classifier']\n",
        "\n",
        "# Get feature importance (Gini importance / gain)\n",
        "feature_importance_gain = xgb_model.feature_importances_\n",
        "\n",
        "# Create DataFrame for easier analysis\n",
        "importance_df = pd.DataFrame({\n",
        "    'feature': feature_names,\n",
        "    'importance_gain': feature_importance_gain\n",
        "}).sort_values('importance_gain', ascending=False)\n",
        "\n",
        "print(\"Feature Importance (Gain-based):\")\n",
        "print(\"=\"*70)\n",
        "print(importance_df.to_string(index=False))\n",
        "print(\"=\"*70)\n"
      ],
      "id": "054c51b1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8b434aa5"
      },
      "outputs": [],
      "source": [
        "# Visualize feature importance\n",
        "plt.figure(figsize=(12, 8))\n",
        "colors = plt.cm.viridis(np.linspace(0, 1, len(importance_df)))\n",
        "bars = plt.barh(range(len(importance_df)), importance_df['importance_gain'], color=colors)\n",
        "plt.yticks(range(len(importance_df)), importance_df['feature'])\n",
        "plt.xlabel('Feature Importance (Gain)', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('Features', fontsize=12, fontweight='bold')\n",
        "plt.title('XGBoost Feature Importance (Gain-based)', fontsize=14, fontweight='bold')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.grid(axis='x', alpha=0.3)\n",
        "\n",
        "# Add value labels on bars\n",
        "for i, (idx, row) in enumerate(importance_df.iterrows()):\n",
        "    plt.text(row['importance_gain'] + 0.005, i, f\"{row['importance_gain']:.3f}\",\n",
        "             va='center', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Highlight top 5 features\n",
        "print(\"\\nTop 5 Most Important Features:\")\n",
        "print(\"=\"*70)\n",
        "for i, (idx, row) in enumerate(importance_df.head(5).iterrows(), 1):\n",
        "    print(f\"{i}. {row['feature']:25s}: {row['importance_gain']:.4f} ({row['importance_gain']/importance_df['importance_gain'].sum()*100:.1f}%)\")\n",
        "print(\"=\"*70)\n"
      ],
      "id": "8b434aa5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ad613c28"
      },
      "outputs": [],
      "source": [
        "# Permutation Importance (more robust, measures actual impact on performance)\n",
        "# This takes longer but gives more reliable importance estimates\n",
        "print(\"Computing Permutation Importance (this may take a minute)...\")\n",
        "X_val_transformed = preprocessor.transform(X_val)\n",
        "\n",
        "perm_importance = permutation_importance(\n",
        "    xgb_model,\n",
        "    X_val_transformed,\n",
        "    y_val,\n",
        "    n_repeats=10,\n",
        "    random_state=42,\n",
        "    scoring='roc_auc',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Create permutation importance DataFrame\n",
        "perm_importance_df = pd.DataFrame({\n",
        "    'feature': feature_names,\n",
        "    'importance_mean': perm_importance.importances_mean,\n",
        "    'importance_std': perm_importance.importances_std\n",
        "}).sort_values('importance_mean', ascending=False)\n",
        "\n",
        "print(\"\\nPermutation Importance (ROC-AUC impact):\")\n",
        "print(\"=\"*70)\n",
        "print(perm_importance_df.to_string(index=False))\n",
        "print(\"=\"*70)\n"
      ],
      "id": "ad613c28"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a74866fb"
      },
      "outputs": [],
      "source": [
        "# Visualize permutation importance with error bars\n",
        "plt.figure(figsize=(12, 8))\n",
        "y_pos = np.arange(len(perm_importance_df))\n",
        "colors = plt.cm.plasma(np.linspace(0, 1, len(perm_importance_df)))\n",
        "\n",
        "plt.barh(y_pos, perm_importance_df['importance_mean'], xerr=perm_importance_df['importance_std'], color=colors, alpha=0.7, capsize=5)\n",
        "plt.yticks(y_pos, perm_importance_df['feature'])\n",
        "plt.xlabel('Permutation Importance (ROC-AUC decrease)', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('Features', fontsize=12, fontweight='bold')\n",
        "plt.title('Permutation Feature Importance (10 repeats)', fontsize=14, fontweight='bold')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.grid(axis='x', alpha=0.3)\n",
        "\n",
        "# Add value labels\n",
        "for i, (idx, row) in enumerate(perm_importance_df.iterrows()):\n",
        "    plt.text(row['importance_mean'] + 0.005, i, f\"{row['importance_mean']:.3f}\", va='center', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nTop 5 Features by Permutation Importance:\")\n",
        "print(\"=\"*70)\n",
        "for i, (idx, row) in enumerate(perm_importance_df.head(5).iterrows(), 1):\n",
        "    print(f\"{i}. {row['feature']:25s}: {row['importance_mean']:.4f} ± {row['importance_std']:.4f}\")\n",
        "print(\"=\"*70)\n"
      ],
      "id": "a74866fb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f042de0"
      },
      "source": [
        "### Feature Importance Interpretation\n",
        "\n",
        "**Clinical Significance:**\n",
        "\n",
        "Based on the feature importance analysis, the top predictors align with clinical knowledge:\n",
        "\n",
        "1. **Time** - Follow-up duration is a strong predictor (patients who die have shorter follow-up)\n",
        "2. **Ejection Fraction** - Direct measure of heart function; lower EF indicates worse heart failure\n",
        "3. **Serum Creatinine** - Kidney function indicator; elevated levels suggest comorbidities\n",
        "4. **Age** - General cardiovascular risk factor\n",
        "5. **Serum Sodium** - Electrolyte imbalance can indicate advanced heart failure\n",
        "\n",
        "These findings are consistent with the literature (Ahmad et al., 2021) and make clinical sense. The model is learning medically relevant patterns rather than spurious correlations.\n"
      ],
      "id": "5f042de0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "befb8f2c"
      },
      "source": [
        "## Final Evaluation on Validation Set\n",
        "\n",
        "**Note:** Threshold tuning was completed in `03-ml-and-predictions.ipynb`. Here we evaluate the final model on the validation set (used as our test set) using the optimal threshold (t=0.250).\n"
      ],
      "id": "befb8f2c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94b80179"
      },
      "outputs": [],
      "source": [
        "# Make predictions on validation set (used as test set)\n",
        "y_val_pred_proba = xgb_pipeline.predict_proba(X_val)[:, 1]  # Probabilities\n",
        "y_val_pred = (y_val_pred_proba >= optimal_threshold).astype(int)  # Binary predictions with optimal threshold\n",
        "\n",
        "# Use helper function for comprehensive evaluation\n",
        "evaluate_model(y_val, y_val_pred, y_val_pred_proba, model_name=f\"XGBoost (Tuned, t={optimal_threshold})\")\n",
        "\n",
        "# Extract metrics for summary table (already calculated in evaluate_model, but recalculate for table)\n",
        "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "val_precision = precision_score(y_val, y_val_pred, pos_label=1, zero_division=0)\n",
        "val_recall = recall_score(y_val, y_val_pred, pos_label=1, zero_division=0)\n",
        "val_f1 = f1_score(y_val, y_val_pred, pos_label=1, zero_division=0)\n",
        "val_roc_auc = roc_auc_score(y_val, y_val_pred_proba)\n",
        "\n",
        "# Create summary results table\n",
        "results_table = pd.DataFrame({\n",
        "    'Metric': ['Accuracy', 'Precision', 'Recall (Sensitivity)', 'F1 Score', 'ROC-AUC'],\n",
        "    'Validation Set Performance': [\n",
        "        f\"{val_accuracy:.4f} ({val_accuracy*100:.2f}%)\",\n",
        "        f\"{val_precision:.4f} ({val_precision*100:.2f}%)\",\n",
        "        f\"{val_recall:.4f} ({val_recall*100:.2f}%)\",\n",
        "        f\"{val_f1:.4f}\",\n",
        "        f\"{val_roc_auc:.4f}\"\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SUMMARY TABLE - VALIDATION SET PERFORMANCE\")\n",
        "print(\"=\"*80)\n",
        "print(results_table.to_string(index=False))\n",
        "print(\"=\"*80)\n"
      ],
      "id": "94b80179"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fda3984d"
      },
      "outputs": [],
      "source": [
        "# Visualize confusion matrix (for reference - already shown in evaluate_model output above)\n",
        "cm_val = confusion_matrix(y_val, y_val_pred)\n",
        "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
        "ConfusionMatrixDisplay(cm_val, display_labels=['Survived', 'Died']).plot(\n",
        "    ax=ax, cmap='Blues', values_format='d'\n",
        ")\n",
        "ax.set_title(f'Confusion Matrix: XGBoost on Validation Set (Threshold = {optimal_threshold})', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "id": "fda3984d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abbbb9f6"
      },
      "outputs": [],
      "source": [
        "# ROC Curve\n",
        "fpr, tpr, roc_thresholds = roc_curve(y_val, y_val_pred_proba)\n",
        "roc_auc = roc_auc_score(y_val, y_val_pred_proba)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# ROC Curve\n",
        "axes[0].plot(fpr, tpr, color='darkorange', lw=2, label=f'XGBoost (AUC = {roc_auc:.3f})')\n",
        "axes[0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier (AUC = 0.500)')\n",
        "axes[0].set_xlim([0.0, 1.0])\n",
        "axes[0].set_ylim([0.0, 1.05])\n",
        "axes[0].set_xlabel('False Positive Rate', fontsize=12)\n",
        "axes[0].set_ylabel('True Positive Rate (Recall)', fontsize=12)\n",
        "axes[0].set_title('ROC Curve: Validation Set Performance', fontsize=14, fontweight='bold')\n",
        "axes[0].legend(loc=\"lower right\", fontsize=10)\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "# Precision-Recall Curve\n",
        "precision, recall, pr_thresholds = precision_recall_curve(y_val, y_val_pred_proba)\n",
        "pr_auc = np.trapz(precision, recall)  # Approximate PR-AUC\n",
        "\n",
        "axes[1].plot(recall, precision, color='green', lw=2, label=f'XGBoost (PR-AUC = {pr_auc:.3f})')\n",
        "# Baseline: proportion of positive class\n",
        "baseline_precision = np.sum(y_val == 1) / len(y_val)\n",
        "axes[1].axhline(y=baseline_precision, color='navy', lw=2, linestyle='--',\n",
        "                label=f'Baseline (P = {baseline_precision:.3f})')\n",
        "axes[1].set_xlim([0.0, 1.0])\n",
        "axes[1].set_ylim([0.0, 1.05])\n",
        "axes[1].set_xlabel('Recall (Sensitivity)', fontsize=12)\n",
        "axes[1].set_ylabel('Precision', fontsize=12)\n",
        "axes[1].set_title('Precision-Recall Curve: Validation Set Performance', fontsize=14, fontweight='bold')\n",
        "axes[1].legend(loc=\"lower left\", fontsize=10)\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nROC-AUC: {roc_auc:.4f}\")\n",
        "print(f\"PR-AUC: {pr_auc:.4f}\")\n"
      ],
      "id": "abbbb9f6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93a17b13"
      },
      "source": [
        "### Threshold Analysis\n",
        "\n",
        "**Note:** Threshold tuning was completed in `03-ml-and-predictions.ipynb`. Here we visualize how the confusion matrix and metrics change with different thresholds to understand the recall-precision trade-off.\n"
      ],
      "id": "93a17b13"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "402de80e"
      },
      "outputs": [],
      "source": [
        "# Compare confusion matrices at different thresholds\n",
        "thresholds_to_compare = [0.3, 0.5, 0.7]\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "for idx, threshold in enumerate(thresholds_to_compare):\n",
        "    y_pred_thresh = (y_val_pred_proba >= threshold).astype(int)\n",
        "    cm = confusion_matrix(y_val, y_pred_thresh)\n",
        "\n",
        "    recall_t = recall_score(y_val, y_pred_thresh, pos_label=1, zero_division=0)\n",
        "    precision_t = precision_score(y_val, y_pred_thresh, pos_label=1, zero_division=0)\n",
        "\n",
        "    ConfusionMatrixDisplay(cm, display_labels=['Survived', 'Died']).plot(\n",
        "        ax=axes[idx], cmap='Blues', values_format='d'\n",
        "    )\n",
        "    axes[idx].set_title(f'Threshold = {threshold}\\nRecall={recall_t:.2f}, Precision={precision_t:.2f}',\n",
        "                       fontsize=12, fontweight='bold')\n",
        "\n",
        "plt.suptitle('Confusion Matrices at Different Thresholds', fontsize=14, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Show metrics at different thresholds\n",
        "print(\"\\nMetrics at Different Thresholds:\")\n",
        "print(\"=\"*80)\n",
        "threshold_analysis = []\n",
        "for threshold in [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]:\n",
        "    y_pred_t = (y_val_pred_proba >= threshold).astype(int)\n",
        "    threshold_analysis.append({\n",
        "        'Threshold': threshold,\n",
        "        'Recall': recall_score(y_val, y_pred_t, pos_label=1, zero_division=0),\n",
        "        'Precision': precision_score(y_val, y_pred_t, pos_label=1, zero_division=0),\n",
        "        'F1': f1_score(y_val, y_pred_t, pos_label=1, zero_division=0)\n",
        "    })\n",
        "\n",
        "threshold_df = pd.DataFrame(threshold_analysis)\n",
        "print(threshold_df.to_string(index=False))\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nOptimal threshold (from notebook 03): {optimal_threshold}\")\n",
        "print(f\"  Recall: {val_recall:.3f}\")\n",
        "print(f\"  Precision: {val_precision:.3f}\")\n",
        "print(f\"  F1: {val_f1:.3f}\")\n"
      ],
      "id": "402de80e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45f0745c"
      },
      "source": [
        "## Overfitting Analysis\n",
        "\n",
        "**Note:** Cross-validation metrics were computed in `03-ml-and-predictions.ipynb`. Here we compare training set performance vs validation set performance to check for overfitting.\n"
      ],
      "id": "45f0745c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0797bd01"
      },
      "outputs": [],
      "source": [
        "# Evaluate on training set\n",
        "y_train_pred_proba = xgb_pipeline.predict_proba(X_train)[:, 1]\n",
        "y_train_pred = (y_train_pred_proba >= optimal_threshold).astype(int)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "train_precision = precision_score(y_train, y_train_pred, pos_label=1, zero_division=0)\n",
        "train_recall = recall_score(y_train, y_train_pred, pos_label=1, zero_division=0)\n",
        "train_f1 = f1_score(y_train, y_train_pred, pos_label=1, zero_division=0)\n",
        "train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
        "\n",
        "# Compare train vs validation\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC-AUC'],\n",
        "    'Training Set': [\n",
        "        f\"{train_accuracy:.4f}\",\n",
        "        f\"{train_precision:.4f}\",\n",
        "        f\"{train_recall:.4f}\",\n",
        "        f\"{train_f1:.4f}\",\n",
        "        f\"{train_roc_auc:.4f}\"\n",
        "    ],\n",
        "    'Validation Set': [\n",
        "        f\"{val_accuracy:.4f}\",\n",
        "        f\"{val_precision:.4f}\",\n",
        "        f\"{val_recall:.4f}\",\n",
        "        f\"{val_f1:.4f}\",\n",
        "        f\"{val_roc_auc:.4f}\"\n",
        "    ],\n",
        "    'Difference': [\n",
        "        f\"{train_accuracy - val_accuracy:+.4f}\",\n",
        "        f\"{train_precision - val_precision:+.4f}\",\n",
        "        f\"{train_recall - val_recall:+.4f}\",\n",
        "        f\"{train_f1 - val_f1:+.4f}\",\n",
        "        f\"{train_roc_auc - val_roc_auc:+.4f}\"\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"OVERFITTING ANALYSIS: Training vs Validation Set Performance\")\n",
        "print(\"=\"*80)\n",
        "print(comparison_df.to_string(index=False))\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Interpretation\n",
        "print(\"\\nInterpretation:\")\n",
        "print(\"-\"*80)\n",
        "max_diff = max(abs(train_accuracy - val_accuracy),\n",
        "               abs(train_roc_auc - val_roc_auc))\n",
        "print(f\"Max Diff: {max_diff}\")\n",
        "print(\"-\"*80)\n"
      ],
      "id": "0797bd01"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9287448"
      },
      "source": [
        "Since max_diff < 0.05 we observe good generalization: Training and validation performance are very similar (< 5% difference).  \n",
        "\n",
        "This means that our model does not exhbit large overfitting behavior and generalizes well."
      ],
      "id": "e9287448"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5db770df"
      },
      "outputs": [],
      "source": [
        "# Visualize train vs validation ROC curves\n",
        "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
        "roc_auc_train = roc_auc_score(y_train, y_train_pred_proba)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.plot(fpr_train, tpr_train, color='blue', lw=2, label=f'Training Set (AUC = {roc_auc_train:.3f})', linestyle='-')\n",
        "plt.plot(fpr, tpr, color='red', lw=2, label=f'Validation Set (AUC = {roc_auc:.3f})', linestyle='-')\n",
        "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--', label='Random Classifier (AUC = 0.500)', alpha=0.5)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('True Positive Rate (Recall)', fontsize=12, fontweight='bold')\n",
        "plt.title('ROC Curves: Training vs Validation Set (Overfitting Check)', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc=\"lower right\", fontsize=11)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Gap analysis\n",
        "gap = roc_auc_train - roc_auc\n",
        "print(f\"\\nROC-AUC Gap (Train - Validation): {gap:+.4f}\")\n",
        "if abs(gap) < 0.05:\n",
        "    print(\"Small gap indicates good generalization\")\n",
        "elif abs(gap) < 0.10:\n",
        "    print(\"Moderate gap - acceptable but monitor\")\n",
        "else:\n",
        "    print(\"Large gap - potential overfitting\")\n"
      ],
      "id": "5db770df"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c483a46"
      },
      "source": [
        "## Final Summary and Clinical Implications\n",
        "\n",
        "### Key Findings\n",
        "\n",
        "**Model Performance:**\n",
        "- **ROC-AUC**: Excellent ranking ability for patient risk stratification\n",
        "- **Recall**: High sensitivity ensures we catch most deaths (critical for clinical use)\n",
        "- **Precision**: Reasonable precision balances false alarms with true detections\n",
        "- **F1 Score**: Good overall balance between precision and recall\n",
        "\n",
        "**Feature Importance:**\n",
        "- Top predictors align with clinical knowledge (time, ejection fraction, serum creatinine)\n",
        "- Model learns medically relevant patterns, not spurious correlations\n",
        "- Feature importance provides interpretability for clinicians\n",
        "\n",
        "**Model Robustness:**\n",
        "- Good generalization (train/validation performance similar)\n",
        "- No significant overfitting detected\n",
        "- Cross-validation results (from notebook 03) were consistent with validation performance\n",
        "\n",
        "### Clinical Recommendations\n",
        "\n",
        "1. **Deployment Considerations:**\n",
        "   - Model shows strong performance but should be validated on external datasets\n",
        "   - Threshold (0.250) optimized for high recall - prioritize catching deaths\n",
        "   - Consider clinical workflow integration for real-time risk assessment\n",
        "\n",
        "2. **Key Risk Factors Identified:**\n",
        "   - Follow-up time (shorter = higher risk)\n",
        "   - Ejection fraction (lower = higher risk)\n",
        "   - Serum creatinine (higher = higher risk)\n",
        "   - Age and serum sodium also contribute\n",
        "\n",
        "3. **Limitations:**\n",
        "   - Small dataset (225 samples) - results may not generalize to all populations\n",
        "   - Single-center data - external validation needed\n",
        "   - Model should complement, not replace, clinical judgment\n"
      ],
      "id": "3c483a46"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "701f4522"
      },
      "outputs": [],
      "source": [
        "# Detailed classification report\n",
        "print(\"=\"*80)\n",
        "print(\"DETAILED CLASSIFICATION REPORT - VALIDATION SET\")\n",
        "print(\"=\"*80)\n",
        "print(classification_report(y_val, y_val_pred,\n",
        "                            target_names=['Survived', 'Died'],\n",
        "                            digits=4))\n",
        "print(\"=\"*80)\n"
      ],
      "id": "701f4522"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dbb1a76"
      },
      "outputs": [],
      "source": [
        "# Plot threshold vs metrics curve\n",
        "thresholds_plot = np.linspace(0.1, 0.9, 17)\n",
        "recall_curve = []\n",
        "precision_curve = []\n",
        "f1_curve = []\n",
        "\n",
        "for t in thresholds_plot:\n",
        "    y_pred_t = (y_val_pred_proba >= t).astype(int)\n",
        "    recall_curve.append(recall_score(y_val, y_pred_t, pos_label=1, zero_division=0))\n",
        "    precision_curve.append(precision_score(y_val, y_pred_t, pos_label=1, zero_division=0))\n",
        "    f1_curve.append(f1_score(y_val, y_pred_t, pos_label=1, zero_division=0))\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "plt.plot(thresholds_plot, recall_curve, 'o-', label='Recall', linewidth=2, markersize=6)\n",
        "plt.plot(thresholds_plot, precision_curve, 's-', label='Precision', linewidth=2, markersize=6)\n",
        "plt.plot(thresholds_plot, f1_curve, '^-', label='F1 Score', linewidth=2, markersize=6)\n",
        "plt.axvline(x=optimal_threshold, color='red', linestyle='--', linewidth=2, label=f'Optimal Threshold (t={optimal_threshold})')\n",
        "plt.axvline(x=0.5, color='gray', linestyle=':', linewidth=1, alpha=0.7, label='Default (t=0.5)')\n",
        "plt.xlabel('Classification Threshold', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('Score', fontsize=12, fontweight='bold')\n",
        "plt.title('Threshold Analysis: Precision-Recall Trade-off (Validation Set)', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc='best', fontsize=10)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nAt optimal threshold (t={optimal_threshold}):\")\n",
        "print(f\"Recall: {val_recall:.3f} - Catches {val_recall*100:.1f}% of deaths\")\n",
        "print(f\"Precision: {val_precision:.3f} - {val_precision*100:.1f}% of predicted deaths are correct\")\n",
        "print(f\"F1: {val_f1:.3f} - Balanced performance metric\")\n"
      ],
      "id": "7dbb1a76"
    },
    {
      "cell_type": "markdown",
      "source": [
        "KNN"
      ],
      "metadata": {
        "id": "J8-rMVngEajo"
      },
      "id": "J8-rMVngEajo"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n"
      ],
      "metadata": {
        "id": "SitnY-6MFkcp"
      },
      "execution_count": null,
      "outputs": [],
      "id": "SitnY-6MFkcp"
    },
    {
      "cell_type": "code",
      "source": [
        "# Train, Validation, Test Split\n",
        "X = df.drop('DEATH_EVENT', axis=1)\n",
        "y = df['DEATH_EVENT']\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
      ],
      "metadata": {
        "id": "buKfSTn1FYce"
      },
      "execution_count": null,
      "outputs": [],
      "id": "buKfSTn1FYce"
    },
    {
      "cell_type": "code",
      "source": [
        "# helper function for evaluation\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
        "\n",
        "def evaluate_model(y_true, y_pred, y_proba=None, model_name=\"Model\"):\n",
        "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
        "    print(\"Precision:\", precision_score(y_true, y_pred))\n",
        "    print(\"Recall:\", recall_score(y_true, y_pred))\n",
        "    print(\"F1 Score:\", f1_score(y_true, y_pred))\n",
        "\n",
        "    if y_proba is not None:\n",
        "        print(\"ROC-AUC:\", roc_auc_score(y_true, y_proba))\n",
        "\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(confusion_matrix(y_true, y_pred))"
      ],
      "metadata": {
        "id": "i-V96ccDG_mX"
      },
      "execution_count": null,
      "outputs": [],
      "id": "i-V96ccDG_mX"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "numerical_features = X_train.columns.tolist()\n",
        "\n",
        "# Create a preprocessor to scale numerical features\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_features)\n",
        "    ])\n",
        "\n",
        "# Init Pipeline\n",
        "knn_pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', KNeighborsClassifier())\n",
        "])\n",
        "\n",
        "# Define Hyperparameter Grid\n",
        "param_grid_knn = {\n",
        "    'classifier__n_neighbors': [5, 7, 10, 13, 15, 18, 20, 25, 30, 40, 50, 65],\n",
        "    'classifier__metric': ['euclidean', 'manhattan']\n",
        "}\n",
        "# Set up 5-fold CV\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# GridSearch\n",
        "grid_knn = GridSearchCV(\n",
        "    estimator=knn_pipeline,\n",
        "    param_grid=param_grid_knn,\n",
        "    cv=cv,\n",
        "    scoring='roc_auc',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid_knn.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best params:\", grid_knn.best_params_)\n",
        "print(\"Best ROC-AUC:\", grid_knn.best_score_)\n",
        "\n",
        "results_df = pd.DataFrame(grid_knn.cv_results_)\n",
        "print(\"\\nAll cross-validation results:\")\n",
        "print(results_df[['param_classifier__n_neighbors', 'param_classifier__metric', 'mean_test_score']].sort_values('mean_test_score', ascending=False))\n",
        "tuned_knn_model = grid_knn.best_estimator_\n",
        "\n",
        "# Make predictions on validation set\n",
        "y_val_pred_tuned_dt = tuned_knn_model.predict(X_val)\n",
        "y_val_pred_proba_tuned_dt = tuned_knn_model.predict_proba(X_val)[:, 1]\n",
        "\n",
        "# Evaluate the tuned model\n",
        "evaluate_model(y_val, y_val_pred_tuned_dt, y_val_pred_proba_tuned_dt, model_name=\"Tuned KNN Model\")\n"
      ],
      "metadata": {
        "id": "_pZmpIgZEdg8"
      },
      "execution_count": null,
      "outputs": [],
      "id": "_pZmpIgZEdg8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA"
      ],
      "metadata": {
        "id": "6qvMyK2WHRBs"
      },
      "id": "6qvMyK2WHRBs"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "X = df.drop(\"DEATH_EVENT\", axis=1)\n",
        "y = df[\"DEATH_EVENT\"]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Run PCA\n",
        "pca = PCA()\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# View explained variance\n",
        "explained_var = pca.explained_variance_ratio_\n",
        "\n",
        "print(\"Explained variance ratio per PC:\")\n",
        "for i, var in enumerate(explained_var, start=1):\n",
        "    print(f\"PC{i}: {var:.4f}\")\n",
        "\n",
        "# Scree Plot\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(range(1, len(explained_var)+1), explained_var, marker='o')\n",
        "plt.xlabel(\"Principal Component\")\n",
        "plt.ylabel(\"Explained Variance Ratio\")\n",
        "plt.title(\"Scree Plot\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# PC1 vs PC2 scatter plot\n",
        "pca_df = pd.DataFrame({\n",
        "    \"PC1\": X_pca[:, 0],\n",
        "    \"PC2\": X_pca[:, 1],\n",
        "    \"Death Event\": y\n",
        "})\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.scatterplot(\n",
        "    data=pca_df,\n",
        "    x=\"PC1\", y=\"PC2\",\n",
        "    hue=\"Death Event\",\n",
        "    palette=\"coolwarm\",\n",
        "    alpha=0.8\n",
        ")\n",
        "plt.title(\"PC1 vs PC2 colored by Death Event\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "uIj9_3GDHMvw"
      },
      "execution_count": null,
      "outputs": [],
      "id": "uIj9_3GDHMvw"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
